{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre_trained_hidden_state_embeddings_Linguistic_simplification_(seq2seq).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "66sASARZOZML",
        "rjxepZkfa7mW",
        "dCe7gl8hPDaq"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob9S213blRMl",
        "colab_type": "text"
      },
      "source": [
        "###Imports and parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0sARhC6egnt",
        "colab_type": "code",
        "outputId": "a80f07e8-cbdb-4387-a0ca-74721ba0ed84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "working_dir = '/content/drive/My Drive/Colab Notebooks/nlp/data/'\n",
        "\n",
        "data_path = working_dir+'sim_dataset_23082019.csv'\n",
        "lm_data_path = working_dir+'he_htb-ud-train.txt'\n",
        "json_dir = working_dir+'lexicon.json'\n",
        "embeddings_file = working_dir+'cc.he.300.vec'\n",
        "results_dir = working_dir+'Results_Pre-trained/'"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwywSbaRJ9Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import requests  \n",
        "#file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.he.300.vec.gz\"\n",
        "    \n",
        "#r = requests.get(file_url, stream = True)  \n",
        "  \n",
        "#with open(\"/content/drive/My Drive/Colab Notebooks/nlp/data/cc.he.300.vec.gz\", \"wb\") as file:  \n",
        "#    for block in r.iter_content(chunk_size = 1024): \n",
        "#         if block:  \n",
        "#             file.write(block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhv2lN1NLnbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! gunzip /content/drive/My\\ Drive/Colab\\ Notebooks/nlp/data/cc.he.300.vec.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZncvdMieiQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy\n",
        "from torch import autograd, nn, optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime\n",
        "#import matplotlib # must be called here for plotting in nova\n",
        "#matplotlib.use('pdf') # must be called here for plotting in nova\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import json\n",
        "import io\n",
        "from nltk.translate import bleu_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yHIdzzcJ1u",
        "colab_type": "text"
      },
      "source": [
        "#### Parameters definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6_ZKLPocJOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 300\n",
        "embedding_size = 300\n",
        "n_layers = 2\n",
        "dropout_p = 0.05\n",
        "teacher_forcing_ratio = 1\n",
        "n_epochs = 30\n",
        "learning_rate = 0.0003\n",
        "\n",
        "print_every = 1\n",
        "print_loss_total = 0 # Reset every print_every\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "fk_belu_train = []\n",
        "fk_belu_val = []\n",
        "dic_reg_to_sim = {}\n",
        "\n",
        "train_LM = False\n",
        "train_LM_n_epochs = 11\n",
        "train_LM_teacher_forcing_ratio = 0.7\n",
        "\n",
        "train_LM_data_size = 6000#00 # 6000 is everything\n",
        "data_size = 6000 # 6000 is everything\n",
        "entires_1_2 = True # use only entires type 1 and 2\n",
        "load_from_pickle = False\n",
        "pickle_python2 = False\n",
        "load_model_flag = False\n",
        "model_params_file = working_dir+\"model_params_pre_trained_n_epochs_\"+str(n_epochs)+\"_LM_\"+str(train_LM_n_epochs)+\".tar\"\n",
        "\n",
        "\n",
        "time = '{date:%Y-%m-%d_%H:%M:%S}'.format( date=datetime.datetime.now() ) \n",
        "if train_LM:\n",
        "  files_suffix = '_Pre-Trained_Embeddings_epoch_%d_LM_epoch_%d_time_%s' %(n_epochs,train_LM_n_epochs,time)\n",
        "else:\n",
        "  files_suffix = '_Pre-Trained_Embeddings_epoch_%d_time_%s' % (n_epochs,time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hOw9LpClYvs",
        "colab_type": "text"
      },
      "source": [
        "###Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66sASARZOZML",
        "colab_type": "text"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yKNnPMUdorH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "UNKNOWN = 2\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "#class Vocab:\n",
        "class Vocab(object):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.index2word = {UNKNOWN:'__unk__'}\n",
        "        self.n_words = 3\n",
        "    \n",
        "    def index_sentence(self, sentence, write=True):\n",
        "        indexes = []\n",
        "        for w in sentence.strip().split(' '):\n",
        "            indexes.append(self.index_word(w, write))\n",
        "        return indexes\n",
        "            \n",
        "    def index_word(self, word, write=True):        \n",
        "        if word not in self.word2index:\n",
        "          if write:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words = self.n_words + 1\n",
        "          else:\n",
        "            return UNKNOWN\n",
        "        return self.word2index[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjxepZkfa7mW",
        "colab_type": "text"
      },
      "source": [
        "#### Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExeSLs986b-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_embeddings(embeddings_file):\n",
        "    fin = io.open(embeddings_file, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
        "    n, d = map(numpy.int32, fin.readline().split())\n",
        "    embeddings = {}\n",
        "    #all_vocabs_words = [unicode(word, \"utf-8\") for word in input_vocab.word2index.keys()+output_vocab.word2index.keys()]\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ') # token[0] is the word itself        \n",
        "        #if tokens[0] in all_vocabs_words:\n",
        "        if (tokens[0] in input_vocab.word2index) or (tokens[0] in output_vocab.word2index):\n",
        "          embeddings[tokens[0]] = torch.tensor(list(map(numpy.float32, tokens[1:]))).unsqueeze(0)\n",
        "#         if tokens[0] in '.':\n",
        "#           embeddings['<EOS>'] = torch.tensor(list(map(numpy.float32, tokens[1:]))).unsqueeze(0)\n",
        "        if tokens[0] in '</s>':\n",
        "          embeddings['<SOS>'] = torch.tensor(list(map(numpy.float32, tokens[1:]))).unsqueeze(0)\n",
        "          embeddings['<EOS>'] = -1*torch.tensor(list(map(numpy.float32, tokens[1:]))).unsqueeze(0)\n",
        "        del tokens\n",
        "    embeddings['unknown'] = torch.ones([300]).unsqueeze(0)/1000\n",
        "    return embeddings "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdputYR-0vJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embeddings(word):\n",
        "  if word in embeddings:\n",
        "    return embeddings[word]\n",
        "  else:\n",
        "    #print('#'+word+'#') # All words suppose to have embeddings\n",
        "    return embeddings['unknown'] # TODO : to check if there is an uknown word, and if not enter one\n",
        "  \n",
        "def get_sentence_embeddings(sent):\n",
        "  emd_list = []\n",
        "  for word in sent.rstrip().split(' '):\n",
        "    emd_list.append(get_embeddings(word))\n",
        "  result = torch.Tensor(len(emd_list), emd_list[0].shape[0])\n",
        "  return torch.cat(emd_list, out=result)\n",
        "\n",
        "def get_word_from_embeddings(word_embedding):\n",
        "#   max_similarity = -float('inf')\n",
        "#   closest_word = 'NO CLOSE WORD'\n",
        "#   for cur_word, cur_emd in embeddings.items():\n",
        "#     cur_simi = F.cosine_similarity(word_embedding,cur_emd.cuda())\n",
        "#     if cur_simi > max_similarity:\n",
        "#       max_similarity = cur_simi\n",
        "#       closest_word = cur_word\n",
        "#   return closest_word\n",
        "  word_embedding = numpy.asarray(word_embedding.detach().cpu())\n",
        "  all_emd = numpy.asarray([numpy.asarray(emd.squeeze(0)) for emd in embeddings.values()])\n",
        "  cos_sim_mat = cosine_similarity(word_embedding,all_emd)\n",
        "  max_sim_list = numpy.argmax(cos_sim_mat, axis=1)\n",
        "  all_words = list(embeddings.keys())\n",
        "  return all_words[max_sim_list[0]]\n",
        "\n",
        "\n",
        "def get_embeddings_similarity_dict(embeddings):\n",
        "  dic = {}\n",
        "  from sklearn.metrics.pairwise import cosine_similarity\n",
        "  all_emd = numpy.asarray([numpy.asarray(emd.squeeze(0)) for emd in embeddings.values()])\n",
        "  cos_sim_mat = cosine_similarity(all_emd,all_emd)\n",
        "  #max_sim_list = numpy.argmax(cos_sim_mat, axis=1)\n",
        "  max_sim_list = numpy.argsort(cos_sim_mat, axis=1)[:,-2] # get second argmax because first argmax is the word itself\n",
        "  \n",
        "  all_words = list(embeddings.keys())\n",
        "  for i, word in enumerate(all_words):\n",
        "    dic[word] = all_words[max_sim_list[i]]\n",
        "  return dic\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H4HKfo0SIa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reverse_and_slice_tensor(tensor, slicing_index):\n",
        "  idx = [i for i in range(tensor.size(0)-1, -1, -1)]\n",
        "  idx = torch.LongTensor(idx[:len(idx)-1-slicing_index])\n",
        "  return tensor.index_select(0, idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W74f7LHuO1Vu",
        "colab_type": "text"
      },
      "source": [
        "#### Read and organize sentances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nh0eJnNdorJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_string(s,only_heb=False):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    if only_heb:\n",
        "      s = re.sub(r\"[\\u0590-\\u05CF]+\", \"\", s)\n",
        "      s = re.sub(r\"[^א-ת.!?]+\", r\" \", s)\n",
        "      s = re.sub(r\"(^|\\s)\\\"(\\w)\", r\"\\1\\2\", re.sub(r\"(\\w)\\\"(\\s|$)\", r\"\\1\\2\", s))\n",
        "    else:\n",
        "      s = re.sub(r\"[^a-zA-Zא-ת.!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "  \n",
        "def filter_pair(p):\n",
        "    not_too_long = len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "    not_too_short = len(p[0].split(' ')) > 1 and len(p[1].split(' ')) > 1\n",
        "    return not_too_long and not_too_short\n",
        "\n",
        "def filter_pairs(pairs):\n",
        "    return [pair for pair in pairs if filter_pair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_LEC2qdorN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_langs(lang1, lang2):    \n",
        "    if 'normal_heb' == lang1 or 'simple_heb' == lang2:\n",
        "      df = pandas.read_csv(data_path, error_bad_lines=False)#, encoding='utf-8')\n",
        "      if entires_1_2:\n",
        "        df = df.loc[df['entry_type']<3]\n",
        "      all_reg_sent = df['reg_sent']\n",
        "      all_sim_sent = df['sim_sent']\n",
        "\n",
        "      pairs = []\n",
        "      for i in list(df.index.values):\n",
        "        pairs.append([normalize_string(all_reg_sent[i],only_heb=True),normalize_string(all_sim_sent[i],only_heb=True)])\n",
        "        \n",
        "    if 'simple-wiki'==lang1 and 'normal-wiki'==lang2:\n",
        "      # Read the file and split into lines\n",
        "      lines = open('/content/drive/My Drive/Colab Notebooks/nlp/data/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
        "\n",
        "      # Split every line into pairs and normalize\n",
        "      lines = [line.split('\\t')[2] for line in lines]\n",
        "      pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "      pairs = [pairs[i]+pairs[i+1] for i in range(0,len(lines),2)]\n",
        "      \n",
        "    if 'lm_train_heb'==lang1 and 'lm_train_heb'==lang2:\n",
        "      lines = open(lm_data_path).read().strip().replace('\\n', ' ').split('. ')\n",
        "      pairs = [[normalize_string(line,only_heb=True),normalize_string(line,only_heb=True)] for line in lines]\n",
        "        \n",
        "    return pairs\n",
        "\n",
        "\n",
        "def print_pair(p):\n",
        "    print(p[0])\n",
        "    print(p[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn-cAjHlS50c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairs_to_data(pairs, size):\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  for pair in pairs[:size]:\n",
        "    if pair[0]!=' ' and pair[1]!= ' ': # delete empty pairs\n",
        "      input_vocab.index_sentence(pair[0])\n",
        "      output_vocab.index_sentence(pair[1])\n",
        "      X.append(pair[0])\n",
        "      y.append(pair[1])\n",
        "  \n",
        "  print(\"Trimmed to %s non-empty sentence pairs\" % len(X))\n",
        "\n",
        "  # Print example\n",
        "  i = random.randint(0,len(X)-1)\n",
        "  print(X[i])\n",
        "  print(y[i])\n",
        "  return numpy.asarray(X), numpy.asarray(y)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCe7gl8hPDaq",
        "colab_type": "text"
      },
      "source": [
        "#### Init data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etOINPOMJoXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_data(input_vocab,output_vocab,dic_reg_to_sim):\n",
        "  #input_vocab, output_vocab, pairs = prepare_data('simple-wiki', 'normal-wiki', True)\n",
        "  \n",
        "  print(\"Readin lexicon...\")\n",
        "  with open(json_dir,'r') as f:\n",
        "\t   dic_reg_to_sim = json.load(f)\n",
        "  \n",
        "  print(\"Reading lines...\")\n",
        "  if train_LM:\n",
        "    #pairs = prepare_data('lm_train_heb', 'lm_train_heb')\n",
        "    pairs = read_langs('lm_train_heb', 'lm_train_heb')\n",
        "    print(\"Read %s sentence pairs for LM\" % len(pairs))    \n",
        "    pairs = filter_pairs(pairs)\n",
        "    X_lm, y_lm = pairs_to_data(pairs, train_LM_data_size)\n",
        "\n",
        "  pairs = read_langs('normal_heb', 'simple_heb')\n",
        "  print(\"Read %s sentence pairs\" % len(pairs))    \n",
        "  pairs = filter_pairs(pairs)\n",
        "  X, y = pairs_to_data(pairs, data_size)\n",
        "  #dic_reg_to_sim = build_dic_reg_to_sim()\n",
        "  print(\"Sentences pairs left:%s\" % len(X))\n",
        "  \n",
        "  print(\"Loading word embeddings...\")\n",
        "  emd = load_embeddings(embeddings_file)\n",
        "  \n",
        "  print(\"Splitting data to train, validtion and test\")\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=1)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)\n",
        "\n",
        "  print(\"Train size:\",len(X_train))\n",
        "  print(\"Validation size:\",len(X_val))\n",
        "  print(\"Test size:\",len(X_test))\n",
        "\n",
        "  print(\"Data is ready!\")\n",
        "  \n",
        "  if train_LM:\n",
        "    return emd, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm\n",
        "  else:\n",
        "    return emd, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXqSuxFclhJT",
        "colab_type": "text"
      },
      "source": [
        "###Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INj8_xGUllls",
        "colab_type": "text"
      },
      "source": [
        "####Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "836gR78kdorX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers        \n",
        "        #self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.bi_grus = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers, batch_first=False, bidirectional=True)\n",
        "\n",
        "    def forward(self, word_inputs, hidden):\n",
        "        seq_len = len(word_inputs)\n",
        "        #embedded = self.embedding(word_inputs).view(seq_len, 1, -1)    \n",
        "        #output, hidden = self.bi_grus(embedded, hidden)\n",
        "        word_inputs = word_inputs.view(seq_len, 1, -1)    \n",
        "        output, hidden = self.bi_grus(word_inputs, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.n_layers*2, 1, self.hidden_size).cuda()\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NiUbmW7ltHN",
        "colab_type": "text"
      },
      "source": [
        "####Decoder with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnsZEQZPdora",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size \n",
        "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        seq_len = len(encoder_outputs)\n",
        "        attn_energies = torch.zeros(seq_len).cuda() # B x 1 x S\n",
        "        \n",
        "        for i in range(seq_len):\n",
        "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
        "\n",
        "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
        "        return F.softmax(attn_energies, 0).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    def score(self, hidden, encoder_output):        \n",
        "        energy = self.attn(encoder_output)        \n",
        "        energy = hidden.view(-1).dot(energy.view(-1))\n",
        "        return energy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0TR9N-Zdord",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_size, output_size, n_layers=1, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        \n",
        "        # Keep parameters for reference\n",
        "        self.hidden_size = hidden_size * 2      \n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "        \n",
        "        # Define layers\n",
        "        #self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(input_size=embedding_size+self.hidden_size, hidden_size=self.hidden_size, num_layers=n_layers, batch_first=False)\n",
        "        self.out = nn.Linear(self.hidden_size * 2, output_size)\n",
        "        self.attn = Attn(self.hidden_size)\n",
        "    \n",
        "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "        \n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        #word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
        "        word_embedded = word_input.view(1, 1, -1) # S=1 x B x N\n",
        "        \n",
        "        # Combine embedded input word and last context, run through RNN \n",
        "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(1)), 2)\n",
        " \n",
        "        #rnn_output, hidden = self.lstm(rnn_input, last_hidden)\n",
        "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
        "\n",
        "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
        "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
        "        \n",
        "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        #output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)), 1)\n",
        "        output = self.out(torch.cat((rnn_output, context), 1))\n",
        "        \n",
        "        # Return final output, hidden state\n",
        "        return output, context, hidden\n",
        "      \n",
        "    def init_hidden(self):\n",
        "      hidden = torch.zeros(self.n_layers, 1, int(self.hidden_size)).cuda()\n",
        "      return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWXWRDbm1t52",
        "colab_type": "text"
      },
      "source": [
        "####Complex words simplifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwKduaMwqZa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simplify(word, lexicon):\n",
        "\t\"\"\"\n",
        "\tAssumes that the word is in the lexicon.\n",
        "\tUses random.choice() since, if there are multiple simplifications, we want to\n",
        "\tpick randomly for them. If the word was simplified in the same way in\n",
        "\tmultiple entries, then the .choice() accounts for the number of time they\n",
        "\toccur.\n",
        "\t:param str word: word to simplify\n",
        "\t:param dict lexicon: dict to map word to simple version\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tsim = random.choice(lexicon[word])[1]\n",
        "\tif type(sim) == str:\n",
        "\t\tsim = [sim]\n",
        "\treturn \" \".join(sim)\n",
        "\n",
        "\n",
        "def get_simple_word_dict(org_sent, target_sent, lexicon, multi_word=0):\n",
        "\t\"\"\"\n",
        "\tDeal with no word in sentence: Pick random shared word. If non exists, pick random.\n",
        "\tDeal with multiple words in sentence: Pick first or Pick random. \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tRandom can be weighted or not.\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tDefault first (multi_word=0)\n",
        "\tIf multiple simplification - Random by weight - do be dealt with simplify() function\n",
        "\tNeed index in target sentence in Train. If simplification not in there, then pick random.\n",
        "\t:param org_sent:\n",
        "\t:param target_sent:\n",
        "\t:param lexicon:\n",
        "\t:param multi_word:\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tpotential = []\n",
        "\torg_sent_split = org_sent.split(' ')[:-1]\n",
        "\ttarget_sent_split = target_sent.split(' ')[:-1]\n",
        "\tfor i, word in enumerate(org_sent_split):\n",
        "\t\tif word in lexicon.keys():\n",
        "\t\t\tpotential.append((i, word))\n",
        "\tif len(potential) > 1:  # Multiple Potential Words to Simplify\n",
        "\t\tif multi_word == 0:  # Heuristic - Pick first word in sentence to simplify\n",
        "\t\t\tsimplified = simplify(potential[0][1], lexicon)  # returns a string (can be multiple words)\n",
        "\t\t\treg_index = potential[0][0]\n",
        "\t\telse:\n",
        "\t\t\t# Option 1 - Random choice with ints\n",
        "\t\t\t# rand_ind = random.randint(0, len(potential) - 1)\n",
        "\t\t\t# simplified = simplify(potential[rand_ind][1], lexicon)  # returns a string (can be multiple words)\n",
        "\t\t\t# reg_index = potential[rand_ind][0]\n",
        "\t\t\t# Option 2 - Random choice with choice\n",
        "\t\t\t# choice = random.choice(potential)\n",
        "\t\t\t# simplified = simplify(choice[1], lexicon)\n",
        "\t\t\t# reg_index = choice[0]\n",
        "\t\t\t# Option 3 - Random choice with choices (weighted\n",
        "\t\t\tws = [len(lexicon[w[1]]) for w in potential]\n",
        "\t\t\tchoice = random.choices(potential, ws)\n",
        "\t\t\tsimplified = simplify(choice[0][1], lexicon)\n",
        "\t\t\treg_index = choice[0][0]\n",
        "\telif len(potential) == 1:  # One potential word to simplify\n",
        "\t\tsimplified = simplify(potential[0][1], lexicon)  # returns a string (can be multiple words)\n",
        "\t\treg_index = potential[0][0]\n",
        "\telse:  # No potential words found\n",
        "\t\ttarget_words = set(target_sent_split) if target_sent is not None else set()\n",
        "\t\tshared_words = (set(org_sent_split) & target_words) - set('.')\n",
        "\t\tsimplified = random.choice(list(shared_words)) if shared_words != set() else random.choice(list(set(org_sent_split) - set('.')))\n",
        "\t\treg_index = org_sent_split.index(simplified)\n",
        "\tif target_sent is not None:  # Training time:\n",
        "\t\tif simplified in target_sent:  # If simplified in sim_sentence, then find index\n",
        "\t\t\tsim_index = target_sent_split.index(simplified.split(' ')[0])  # index of first word of simplified\n",
        "\t\telse:  # else pick random index\n",
        "\t\t\tsim_index = random.randint(0, len(target_sent_split) - 1)\n",
        "\t\treturn simplified, reg_index, sim_index  # simplified, index in org_sent, index in target_sent\n",
        "\telse:  # Test time:\n",
        "\t\treturn simplified, reg_index  # simplified, index in org_sent\n",
        "\n",
        "\n",
        "def get_simple_word_rand(org_sent, target_sent=None):\n",
        "\tif target_sent is None:  # Test time\n",
        "\t\torg_index = random.randint(0, len(org_sent.split(' '))-1)\n",
        "\t\treturn org_sent.split(' ')[org_index], org_index  # simplified, index in org_sent\n",
        "\telse:  # Train time\n",
        "\t\torg_index = random.randint(0, len(org_sent.split(' '))-1)\n",
        "\t\ttarget_index = random.randint(0, len(target_sent.split(' '))-1)\n",
        "\t\treturn target_sent.split(' ')[target_index], org_index, target_index  # simplified, index in org_sent, index in target_sent\n",
        "\n",
        "\n",
        "def get_simple_word(org_sent, target_sent=None, get_kind=0, lex=dic_reg_to_sim):\n",
        "\tassert type(get_kind) == int and get_kind < 4\n",
        "\tif get_kind == 0:\n",
        "\t\treturn get_simple_word_rand(org_sent, target_sent)\n",
        "\telif get_kind == 1:\n",
        "\t\treturn get_simple_word_dict(org_sent, target_sent, lexicon=lex)\n",
        "\telif get_kind == 2:\n",
        "\t\tpass\n",
        "\t\t# return get_simple_word_classifier(org_sent, target_sent=None, lexicon=lex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB3Yr5xu7ity",
        "colab_type": "text"
      },
      "source": [
        "#### Save and Load models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sd3GR8nc7hha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(epoch):\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'forward_decoder_state_dict': forward_decoder.state_dict(),\n",
        "            'backward_decoder_state_dict': backward_decoder.state_dict(),\n",
        "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
        "            'forward_decoder_optimizer_state_dict': forward_decoder_optimizer.state_dict(),\n",
        "            'backward_decoder_optimizer_state_dict': backward_decoder_optimizer.state_dict(),     \n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'files_suffix': files_suffix,\n",
        "            }, model_params_file)\n",
        "  print(\"Saved file:'\"+model_params_file+\" for epoch:\",str(epoch))\n",
        "\n",
        "\n",
        "def load_model():\n",
        "  device = torch.device(\"cuda\")\n",
        "#   encoder = EncoderRNN(input_vocab.n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "#   forward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "#   backward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "#   encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "#   forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate)\n",
        "#   backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate)\n",
        "  # Initialize models\n",
        "  if pickle_python2:\n",
        "    encoder = EncoderRNN(input_vocab_n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "  else:\n",
        "    encoder = EncoderRNN(input_vocab.n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "  forward_decoder = AttnDecoderRNN(hidden_size, embedding_size, embedding_size, n_layers, dropout_p=dropout_p).cuda()\n",
        "  backward_decoder = AttnDecoderRNN(hidden_size, embedding_size, embedding_size, n_layers, dropout_p=dropout_p).cuda()\n",
        "  # Initialize optimizers\n",
        "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "  forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate)\n",
        "  backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "  checkpoint = torch.load(model_params_file)\n",
        "  \n",
        "  encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "  forward_decoder.load_state_dict(checkpoint['forward_decoder_state_dict'])\n",
        "  backward_decoder.load_state_dict(checkpoint['backward_decoder_state_dict'])\n",
        "  \n",
        "  encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
        "  forward_decoder_optimizer.load_state_dict(checkpoint['forward_decoder_optimizer_state_dict'])\n",
        "  backward_decoder_optimizer.load_state_dict(checkpoint['backward_decoder_optimizer_state_dict'])\n",
        "  \n",
        "  epoch = checkpoint['epoch']\n",
        "  train_losses = checkpoint['train_losses']\n",
        "  val_losses = checkpoint['val_losses']\n",
        "  files_suffix = checkpoint['files_suffix']\n",
        "  \n",
        "  encoder.to(device)\n",
        "  forward_decoder.to(device)\n",
        "  backward_decoder.to(device)\n",
        "  encoder.train()\n",
        "  forward_decoder.train()\n",
        "  backward_decoder.train()\n",
        "  \n",
        "  return encoder,forward_decoder,backward_decoder,encoder_optimizer,forward_decoder_optimizer,backward_decoder_optimizer,epoch, train_losses, val_losses, files_suffix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpkz7V-Ppoc7",
        "colab_type": "text"
      },
      "source": [
        "### Train and Evaluation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1tqRoAmJHm",
        "colab_type": "text"
      },
      "source": [
        "####Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFL0an_dorj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_tensor, backward_target_tensor, forward_target_tensor, simplified_word, forward_decoder, backward_decoder,\n",
        "          encoder_optimizer, forward_decoder_optimizer, backward_decoder_optimizer, criterion, teacher_forcing_ratio=1, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients of both optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    backward_decoder_optimizer.zero_grad()\n",
        "    forward_decoder_optimizer.zero_grad()\n",
        "    loss = 0 # Added onto for each word\n",
        "    \n",
        "    # Get size of input and target sentences\n",
        "    input_length = input_tensor.size()[0]\n",
        "    backward_target_length = backward_target_tensor.size()[0]\n",
        "    forward_target_length = forward_target_tensor.size()[0]\n",
        "    \n",
        "    loss += run_model(input_tensor, backward_target_tensor, forward_target_tensor, input_length, backward_target_length, forward_target_length,\n",
        "                      simplified_word, backward_decoder, forward_decoder, criterion)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    backward_decoder_optimizer.step()\n",
        "    forward_decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item() / (backward_target_length+forward_target_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um5KixbaJYMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(input_tensor, backward_target_tensor, forward_target_tensor, input_length, backward_target_length, forward_target_length, simplified_word, backward_decoder,\n",
        "              forward_decoder, loss_func, teacher_forcing_ratio=teacher_forcing_ratio): \n",
        "  loss = 0 # Added onto for each word\n",
        "\n",
        "  # Run words through encoder\n",
        "  encoder_hidden = encoder.init_hidden()\n",
        "  encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "  \n",
        "  # Prepare input and output variables\n",
        "  #forward_decoder_input = torch.LongTensor([[output_vocab.index_word(simplified_word, write=False)]]).cuda() #forward pass from y_s\n",
        "  #forward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "  #backward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "  #backward_decoder_input = torch.LongTensor([[output_vocab.index_word(simplified_word, write=False)]]).cuda()\n",
        "  forward_decoder_input = simplified_word.cuda() #forward pass from y_s\n",
        "  forward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "  backward_decoder_input = simplified_word.cuda()\n",
        "  backward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "\n",
        "  backward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder\n",
        "  #change here #forward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder\n",
        "\n",
        "  # Run model on input\n",
        "\n",
        "  # Choose whether to use teacher forcing (Teacher forcing: Use the ground-truth target as the next input)\n",
        "  use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "  # Backward pass\n",
        "  for di in range(backward_target_length):\n",
        "      backward_decoder_output, backward_decoder_context, backward_decoder_hidden = backward_decoder(\n",
        "          backward_decoder_input, backward_decoder_context, backward_decoder_hidden, encoder_outputs)\n",
        "      target_lables = torch.ones([1,300]).cuda()\n",
        "      loss += loss_func(backward_decoder_output[0].view(-1).unsqueeze(0), backward_target_tensor[di].unsqueeze(0),target_lables)\n",
        "      if use_teacher_forcing:\n",
        "        backward_decoder_input = backward_target_tensor[di] # Next target is next input\n",
        "      else:\n",
        "        next_word = get_word_from_embeddings(backward_decoder_output.cuda())\n",
        "        #backward_decoder_input = get_embeddings(next_word).cuda()\n",
        "        backward_decoder_input = get_embeddings(next_word).cuda()\n",
        "\n",
        "        # Stop at start of sentence (not necessary when using known targets)\n",
        "        if next_word == '<SOS>': break\n",
        "\n",
        "  # Forward pass\n",
        "  forward_decoder_hidden = backward_decoder_hidden # change here\n",
        "  for di in range(forward_target_length):\n",
        "      forward_decoder_output, forward_decoder_context, forward_decoder_hidden = forward_decoder(\n",
        "          forward_decoder_input, forward_decoder_context, forward_decoder_hidden, encoder_outputs)\n",
        "      target_lables = torch.ones([1,300]).cuda()\n",
        "      loss += loss_func(forward_decoder_output[0].view(-1).unsqueeze(0), forward_target_tensor[di].unsqueeze(0),target_lables)\n",
        "      if use_teacher_forcing:\n",
        "        forward_decoder_input = forward_target_tensor[di] # Next target is next input\n",
        "      else:\n",
        "        next_word = get_word_from_embeddings(forward_decoder_output.cuda())\n",
        "        forward_decoder_input = get_embeddings(next_word).cuda()\n",
        "        # Stop at start of sentence (not necessary when using known targets)\n",
        "        if next_word == '<EOS>': break\n",
        "\n",
        "    \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeONurq3qD-e",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzV2YNg7SwDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_val_loss(backward_decoder, forward_decoder,teacher_forcing_ratio=0):\n",
        "    backward_decoder.eval()\n",
        "    forward_decoder.eval()\n",
        "    encoder.eval()\n",
        "    val_loss_sum = 0\n",
        "    fk_belu = 0\n",
        "    \n",
        "    for i in range(len(X_val)):    \n",
        "      pair = (X_val[i],y_val[i])         \n",
        "      # Finding the complex word x_c and setting simple word y_s    \n",
        "      simplified_word, org_index , target_index = get_simple_word(pair[0],pair[1]) \n",
        "#       input_tensor = torch.tensor([SOS_TOKEN]+(input_vocab.index_sentence(pair[0], write=False))+[EOS_TOKEN]).cuda()\n",
        "#       backward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index-1::-1])+[SOS_TOKEN]).cuda() # from y_s-1 to SOS\n",
        "#       forward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index+1:])+[EOS_TOKEN]).cuda() # from y_s+1 to EOS\n",
        "      simplified_word = get_embeddings(simplified_word)\n",
        "      input_tensor =  torch.cat((get_embeddings('<SOS>'), torch.cat((get_sentence_embeddings(pair[0]),get_embeddings('<EOS>'))))).cuda()\n",
        "      backward_target_tensor = torch.cat((reverse_and_slice_tensor(get_sentence_embeddings(pair[1]), target_index), get_embeddings('<SOS>'))).cuda() # from y_s-1 to SOS\n",
        "      forward_target_tensor = torch.cat((get_sentence_embeddings(pair[1])[target_index+1:],get_embeddings('<EOS>'))).cuda() # from y_s+1 to EOS\n",
        "\n",
        "      # Get size of input and target sentences\n",
        "      input_length = input_tensor.size()[0]\n",
        "      backward_target_length = backward_target_tensor.size()[0]\n",
        "      forward_target_length = forward_target_tensor.size()[0]\n",
        "      \n",
        "      cur_val_loss = run_model(input_tensor, backward_target_tensor, forward_target_tensor, input_length, backward_target_length, forward_target_length,\n",
        "                               simplified_word, backward_decoder, forward_decoder, nn.CosineEmbeddingLoss())\n",
        "      val_loss_sum += cur_val_loss.item() / (backward_target_length+forward_target_length) \n",
        "      \n",
        "      #fk_belu += FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], evaluate(pair[0])[1:-1])\n",
        "    \n",
        "    backward_decoder.train()\n",
        "    forward_decoder.train()\n",
        "    encoder.train()\n",
        "    \n",
        "    return val_loss_sum/len(X_val), fk_belu/len(X_val)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqBumV5TMyp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs, max_length=MAX_LENGTH, is_backward=None):\n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length, max_length).cuda()\n",
        "    \n",
        "    # Run through decoder\n",
        "    for di in range(max_length):\n",
        "        if is_backward:\n",
        "          decoder_output, decoder_context, decoder_hidden = backward_decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
        "        else:\n",
        "          decoder_output, decoder_context, decoder_hidden = forward_decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
        "        # Choose top word from output\n",
        "        next_word = get_word_from_embeddings(decoder_output.cuda())\n",
        "        decoded_words.append(next_word)\n",
        "        \n",
        "        if (next_word == '<SOS>' and is_backward) or (next_word == '<EOS>' and not is_backward):\n",
        "            break\n",
        "            \n",
        "        # Next input is chosen word\n",
        "        decoder_input = get_embeddings(next_word).cuda()\n",
        "        \n",
        "    return decoded_words\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivPXrB-idor1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence, max_length=MAX_LENGTH):\n",
        "      \n",
        "    #return sentence.strip().split(' ')\n",
        "    backward_decoder.eval()\n",
        "    forward_decoder.eval()\n",
        "    encoder.eval()\n",
        "    \n",
        "    #input_tensor = torch.tensor(input_vocab.index_sentence(sentence)).cuda()\n",
        "    input_tensor =  torch.cat((get_embeddings('<SOS>'), torch.cat((get_sentence_embeddings(sentence),get_embeddings('<EOS>'))))).cuda()\n",
        "    input_length = input_tensor.size()[0]\n",
        "    \n",
        "    # Run through encoder\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "    \n",
        "    # Finding the complex word x_c and setting simple word y_s    \n",
        "    simplified_word, org_index = get_simple_word(sentence)   \n",
        "    simplified_word_embd = get_embeddings(simplified_word)\n",
        "    \n",
        "    # Create starting vectors for backward decoder\n",
        "    backward_decoder_input = simplified_word_embd.cuda() # y_s\n",
        "    backward_decoder_context = torch.zeros(1, backward_decoder.hidden_size).cuda()\n",
        "    backward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder\n",
        "    \n",
        "    # Create starting vectors for forward decoder\n",
        "    forward_decoder_input = simplified_word_embd.cuda() # SOS\n",
        "    forward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "    forward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder\n",
        "    \n",
        "    # Run through decoders\n",
        "    backward_words = run_decoder(backward_decoder_input, backward_decoder_context, backward_decoder_hidden, encoder_outputs, max_length, is_backward=True)\n",
        "    forward_words = run_decoder(forward_decoder_input, forward_decoder_context, forward_decoder_hidden, encoder_outputs, max_length, is_backward=False)\n",
        "    \n",
        "    # Bulid sentance\n",
        "    decoded_words = backward_words[::-1]+[simplified_word]+forward_words\n",
        "    \n",
        "    backward_decoder.train()\n",
        "    forward_decoder.train()\n",
        "    encoder.train()\n",
        "    \n",
        "    return decoded_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExP97mC2dor5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_randomly():\n",
        "    #pair = random.choice(pairs)\n",
        "    i = random.randint(0,len(X_val)-1)\n",
        "    pair = [X_val[i],y_val[i]]\n",
        "  \n",
        "    output_words = evaluate(pair[0])\n",
        "    print(\"pair\",pair)\n",
        "    print(\"output_words\",output_words)\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    \n",
        "    print(\"Validation example\")\n",
        "    print('FKBLEU: ', FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], output_words[1:-1]))\n",
        "    if pickle_python2:\n",
        "      print(pair[0].encode('utf-8'))\n",
        "      print(pair[1].encode('utf-8'))\n",
        "      print(output_sentence.encode('utf-8'))\n",
        "    else:\n",
        "      print(pair[0])\n",
        "      print(pair[1])\n",
        "      print(output_sentence)\n",
        "    \n",
        "def evaluate_randomly_train():\n",
        "    #pair = random.choice(pairs)\n",
        "    i = random.randint(0,len(X_val)-1)\n",
        "    pair = [X_train[i],y_train[i]]\n",
        "  \n",
        "    output_words = evaluate(pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    \n",
        "    print(\"Train example\")\n",
        "    print('FKBLEU: ', FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], output_words[1:-1]))\n",
        "    if pickle_python2:\n",
        "      print(pair[0].encode('utf-8'))\n",
        "      print(pair[1].encode('utf-8'))\n",
        "      print(output_sentence.encode('utf-8'))\n",
        "    else:\n",
        "      print(pair[0])\n",
        "      print(pair[1])\n",
        "      print(output_sentence)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dJ_RbGmDGzP",
        "colab_type": "text"
      },
      "source": [
        "#### FK BELU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qv2yfZnDHOk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iBLEU(input_sent, reference, candidate, alpha=0.9):\n",
        "\t\"\"\"\n",
        "\tCalculate iBLEU according to Xu et. al. 2016\n",
        "\t:param input_sent: original sentence\n",
        "\t:param reference: the target sentences to test by\n",
        "\t:param candidate: a proposed sentence from the input\n",
        "\t:param alpha: default param 0.9 from Sun and Zhou (2012)\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tsmooth = bleu_score.SmoothingFunction()\n",
        "\tif len(candidate) < 2:\n",
        "\t\treturn 0.0\n",
        "\tref_candidate = bleu_score.sentence_bleu(reference, candidate, smoothing_function=smooth.method7)\n",
        "\tinput_candidate = bleu_score.sentence_bleu(input_sent, candidate, smoothing_function=smooth.method7)\n",
        "\t# print('ref_candidate:', '%s; ' % ref_candidate, 'input_candidate:','%s; ' % input_candidate)\n",
        "\t# print('iBLEU (alpha * ref_candidate - (1 - alpha) * input_candidate) =\\n %s' % (alpha * ref_candidate - (1 - alpha) * input_candidate))\n",
        "\treturn alpha * ref_candidate - (1 - alpha) * input_candidate\n",
        "\n",
        "\t\n",
        "\n",
        "def FK(text, language='heb'):\n",
        "\t\"\"\"\n",
        "\tTODO: count syllables not with heuristic\n",
        "\tCalculate Flesch-Kincaid Index (Kincaid et al 1975) according to Xu et. al. 2016\n",
        "\t:param language: Used for syllables count heuristic\n",
        "\t:param text: Assumes is a list of lists of words (list of sentences as lists of words)\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\t# if isinstance(text, list) and all(isinstance(sen, list) for sen in text):\n",
        "\t# \tfor sen in text:\n",
        "\t# \t\tassert all(isinstance(w, str) for w in sen)\n",
        "\t\n",
        "\tnum_words = 0\n",
        "\tnum_sents = 0\n",
        "\tnum_syllables = 0\n",
        "\tif language == 'eng':\n",
        "\t\tparser = pyphen.Pyphen('en_us')\n",
        "\telse:\n",
        "\t\tparser = None\n",
        "\t# Gather numerical calculations\n",
        "\tfor sen in text:\n",
        "\t\tnum_sents += 1\n",
        "\t\tfor word in sen:\n",
        "\t\t\tnum_words += 1\n",
        "\t\t\tif language == 'heb':  # heuristic that each letter is a syllable in hebrew\n",
        "\t\t\t\tnum_syllables += len(word)\n",
        "\t\t\telif language == 'eng':  # syllable parser for English\n",
        "\t\t\t\tnum_syllables += len(parser.inserted(word).split('-'))\n",
        "\t#print('Words, Sents, Syllables: %s, %s, %s' % (num_words, num_sents, num_syllables))\n",
        "\treturn 0.39 * (num_words / num_sents) + 11.8 * (num_syllables / num_words) - 15.59\n",
        "\n",
        "\n",
        "def FKdiff(input_sent, candidate):\n",
        "  \"\"\"\n",
        "  \n",
        "  :param input_sent: Assumes input sent is a list of lists of words\n",
        "  :param candidate: Assumes candidate is a list of words\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  # using torch\n",
        "  # print('FKdiff:', torch.sigmoid(FK([candidate]) - FK(input_sent)))\n",
        "  # return torch.nn.functional.sigmoid(FK([candidate]) - FK(input_sent))\n",
        "  # using python native\n",
        "  x = FK([candidate]) - FK(input_sent)\n",
        "  # print('FK(candidate): %s;  FK(input_sent): %s' % (FK([candidate]), FK(input_sent)))\n",
        "  # print('x: %s' % x)\n",
        "  # print('FKdiff:', 1 / (1 + numpy.exp(-x)))\n",
        "  return 1 / (1 + numpy.exp(-x))\n",
        "\n",
        "\n",
        "def FKBLEU(input_sent, references, candidate):\n",
        "\t\"\"\"\n",
        "\tCalculate iBLEU according to Xu et. al. 2016\n",
        "\t:param input_sent: original sentence. Assumes list of lists of words\n",
        "\t:param references: the target sentences to test by. Assumes list of lists of words\n",
        "\t:param candidate: a proposed sentence from the input. List of words.\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\t#print('Input: %s, refrences: %s, candidate: %s' % (input_sent, references, candidate))\n",
        "\treturn iBLEU(input_sent, references, candidate) * FKdiff(input_sent, candidate)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiQAHpxEqxzW",
        "colab_type": "text"
      },
      "source": [
        "#### Create results files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg65Xlp2qi25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write results to file\n",
        "def evaluate_validation():\n",
        "  fk_belu = 0\n",
        "  i_belu = 0\n",
        "  fk = 0\n",
        "  f = open(results_dir+'Results'+files_suffix+'.txt', 'w+')\n",
        "  \n",
        "  for i in range(len(X_val)):\n",
        "    pair = [X_val[i],y_val[i]]\n",
        "    output_words = evaluate(pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    f.write(\"This is line %d\\r\\n\" % (i+1))\n",
        "    \n",
        "    if pickle_python2:\n",
        "      f.write(pair[0].encode('utf-8')+\" \\n\")\n",
        "      f.write(pair[1].encode('utf-8')+\" \\n\")\n",
        "      f.write(output_sentence.encode('utf-8')+\" \\n\")\n",
        "    else:\n",
        "      f.write(pair[0]+\" \\n\")\n",
        "      f.write(pair[1]+\" \\n\")\n",
        "      f.write(output_sentence+\" \\n\")\n",
        "    \n",
        "\n",
        "    input_sent = [pair[0].split(' ')]\n",
        "    references = [pair[1].split(' ')]\n",
        "    candidate = output_words[1:-1]\n",
        "    fk_belu += FKBLEU(input_sent, references , candidate)\n",
        "    i_belu += iBLEU(input_sent, references, candidate)\n",
        "    fk += FK(candidate)\n",
        "  print(\"###########################################\")\n",
        "  print(\"\\n This is FK-BELU on the validation %.18f \\n\" % (fk_belu/len(X_val)))\n",
        "  print(\"\\n This is iBELU on the validation %.18f \\n\" % (i_belu/len(X_val)))\n",
        "  print(\"\\n This is FK on the validation %.18f \\n\" % (fk/len(X_val)))\n",
        "  f.write(\"\\n This is FK-BELU on the validation %.18f \\n\" % (fk_belu/len(X_val)))\n",
        "  f.write(\"\\n This is i_belu on the validation %.18f \\n\" % (i_belu/len(X_val)))\n",
        "  f.write(\"\\n This is FK on the validation %.18f \\n\" % (fk/len(X_val)))\n",
        "  f.write(\"\\n Those are the run parameters: LR %.5f, dropout %.5f, n_layers %d, embedding_size %d,hidden_size %d \\n\" % (learning_rate,dropout_p,n_layers,embedding_size,hidden_size))\n",
        "  print(\"Wrote those results to file\")\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5fSU8ZGq2Z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Plotter:\n",
        "    def __init__(self, first_losses, fist_label, sec_losses, sec_label):\n",
        "        self.first_losses = first_losses\n",
        "        self.fist_label = fist_label\n",
        "        self.sec_losses = sec_losses\n",
        "        self.sec_label = sec_label\n",
        "\n",
        "    def plot(self, file_name):\n",
        "        \"\"\"Plot the loss per epoch\"\"\"\n",
        "        line1, = plt.plot(range(len(self.first_losses)), self.first_losses, label=self.fist_label)\n",
        "        line2, = plt.plot(range(len(self.sec_losses)), self.sec_losses, label=self.sec_label)\n",
        "        plt.legend(handles=[line1,line2])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Cosine Embedding Loss')\n",
        "        plt.grid(True)\n",
        "        plt.savefig(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3nJX5Wclz_m",
        "colab_type": "text"
      },
      "source": [
        "###Running models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4uUKZcSmMa3",
        "colab_type": "text"
      },
      "source": [
        "####Models' init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sVCDg_3doro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "40eb1939-be10-42b5-e61d-f271c0ab55e8"
      },
      "source": [
        "# Data Load\n",
        "\n",
        "try:\n",
        "  if not load_from_pickle:\n",
        "    raise IOError(\"Do not load from pickle!\")\n",
        "  if train_LM:\n",
        "    if pickle_python2:\n",
        "      embeddings, input_vocab_n_words, output_vocab_n_words, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm = pickle.load(open(working_dir+\"pre_trained_data_with_LM_python2.pickle\", \"rb\"))\n",
        "    else:\n",
        "      embeddings, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm = pickle.load(open(working_dir+\"pre_trained_data_with_LM.pickle\", \"rb\"))\n",
        "  else:\n",
        "    if pickle_python2:\n",
        "      embeddings, input_vocab_n_words, output_vocab_n_words, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(open(working_dir+\"pre_trained_data_python2.pickle\", \"rb\"))\n",
        "    else:\n",
        "      embeddings, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(open(working_dir+\"pre_trained_data.pickle\", \"rb\"))\n",
        "except (OSError, IOError) as e:\n",
        "  input_vocab = Vocab(\"Heb-reg\")\n",
        "  output_vocab = Vocab(\"Heb-simple\")\n",
        "  dic_reg_to_sim = {}\n",
        "  if train_LM:\n",
        "    embeddings, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm = init_data(input_vocab,output_vocab,dic_reg_to_sim)\n",
        "    if pickle_python2:\n",
        "      pickle.dump([embeddings, input_vocab.n_words, output_vocab.n_words, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm], open(working_dir+\"pre_trained_data_with_LM_python2.pickle\", \"wb\"), protocol=2)\n",
        "    else:\n",
        "      pickle.dump([embeddings, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm], open(working_dir+\"pre_trained_data_with_LM.pickle\", \"wb\"))\n",
        "    \n",
        "  else:\n",
        "    embeddings, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test = init_data(input_vocab,output_vocab,dic_reg_to_sim)\n",
        "    if pickle_python2:\n",
        "      pickle.dump([embeddings, input_vocab.n_words, output_vocab.n_words, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test], open(working_dir+\"pre_trained_data_python2.pickle\", \"wb\"), protocol=2)\n",
        "    else:\n",
        "      pickle.dump([embeddings, input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test], open(working_dir+\"pre_trained_data.pickle\", \"wb\"))   \n",
        "    \n",
        "  print(\"Pickled all data\")\n",
        "\n",
        "  \n",
        "# Initialize models\n",
        "if pickle_python2:\n",
        "  encoder = EncoderRNN(input_vocab_n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "else:\n",
        "  encoder = EncoderRNN(input_vocab.n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "# forward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "# backward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "forward_decoder = AttnDecoderRNN(hidden_size, embedding_size, embedding_size, n_layers, dropout_p=dropout_p).cuda()\n",
        "backward_decoder = AttnDecoderRNN(hidden_size, embedding_size, embedding_size, n_layers, dropout_p=dropout_p).cuda()\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate)\n",
        "backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate)\n",
        "# criterion = nn.NLLLoss()\n",
        "criterion = nn.CosineEmbeddingLoss()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Readin lexicon...\n",
            "Reading lines...\n",
            "Read 1406 sentence pairs\n",
            "Trimmed to 1062 non-empty sentence pairs\n",
            "והנה מישהו שבטוח שהיה רוצה להוסיף מנצח אירוויזיון לקורות החיים שלו .\n",
            "ופה יש מישהו שבטח ישמח לזכות באירוויזיון .\n",
            "Sentences pairs left:1062\n",
            "Loading word embeddings...\n",
            "Splitting data to train, validtion and test\n",
            "Train size: 907\n",
            "Validation size: 101\n",
            "Test size: 54\n",
            "Data is ready!\n",
            "Pickled all data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5hGCEk_l_Pb",
        "colab_type": "text"
      },
      "source": [
        "####Training as LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LcrHj6HSdort",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train_LM:\n",
        "  print(\"Training model as LM...\")\n",
        "  \n",
        "  if load_model_flag:\n",
        "    print(\"Loading model's parameters in LM...\")\n",
        "    encoder,forward_decoder,backward_decoder,encoder_optimizer,forward_decoder_optimizer,backward_decoder_optimizer,epoch, train_losses, val_losses, files_suffix = load_model()\n",
        "    print(\"Training LM from epoch: \",epoch)\n",
        "  else:\n",
        "    epoch = 1\n",
        "  \n",
        "  f = open(results_dir+'Train_LM_loss_'+files_suffix+'.txt', 'w+')\n",
        "  \n",
        "  while epoch < train_LM_n_epochs + 1:\n",
        "      for i in range(len(X_lm)):\n",
        "        pair = (X_lm[i],y_lm[i])        \n",
        "        splitting_index = (len(pair[0].split(' '))-1)//2 # mid sentance so both decoders would learn to generate words\n",
        "        \n",
        "        #simplified_word = output_vocab.index_word(pair[1].split(' ')[splitting_index])\n",
        "        simplified_word = get_embeddings(pair[1].split(' ')[splitting_index])\n",
        "\n",
        "#         input_tensor = torch.tensor([SOS_TOKEN]+(input_vocab.index_sentence(pair[0], write=False))+[EOS_TOKEN]).cuda()\n",
        "#         backward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[splitting_index-1::-1])+[SOS_TOKEN]).cuda() # from y_s-1 to SOS\n",
        "#         forward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[splitting_index+1:])+[EOS_TOKEN]).cuda() # from y_s+1 to EOS\n",
        "        \n",
        "        input_tensor =  torch.cat((get_embeddings('<SOS>'), torch.cat((get_sentence_embeddings(pair[0]),get_embeddings('<EOS>'))))).cuda()\n",
        "        backward_target_tensor = torch.cat((reverse_and_slice_tensor(get_sentence_embeddings(pair[1]), splitting_index), get_embeddings('<SOS>'))).cuda() # from y_s-1 to SOS\n",
        "        forward_target_tensor = torch.cat((get_sentence_embeddings(pair[1])[splitting_index+1:],get_embeddings('<EOS>'))).cuda() # from y_s+1 to EOS\n",
        "\n",
        "        # Run the train function\n",
        "        loss = train(input_tensor, backward_target_tensor, forward_target_tensor, simplified_word, forward_decoder, backward_decoder,\n",
        "                     encoder_optimizer, forward_decoder_optimizer, backward_decoder_optimizer, criterion,train_LM_teacher_forcing_ratio)\n",
        "\n",
        "        # Keep track of loss\n",
        "        print_loss_total += loss\n",
        "        if i == 0: continue\n",
        "          \n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print_loss_avg = print_loss_total / i\n",
        "            print('(Epoch %d)(pair number %d) %.4f' % (epoch,i, print_loss_avg))\n",
        "\n",
        "      if epoch == 0: continue\n",
        "\n",
        "      if epoch % print_every == 0:\n",
        "          print_loss_avg = print_loss_total / (print_every * len(X_lm))\n",
        "          print_loss_total = 0\n",
        "          print('(Epoch %d) %.4f' % (epoch, print_loss_avg))\n",
        "          f.write(\"(Epoch %d) %.4f \\n\" % (epoch, print_loss_avg))\n",
        "      epoch += 1\n",
        "      save_model(epoch)\n",
        "\n",
        "  print(\"Done!\")\n",
        "  f.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc0Lg9aPT6p3",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7iqEVOWT5OM",
        "colab_type": "code",
        "outputId": "11506df9-c188-43af-933f-94b847d27c05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Begin!\n",
        "\n",
        "print(\"Training...\")\n",
        "\n",
        "if load_model_flag and not train_LM:\n",
        "  print(\"Loading model's parameters...\")\n",
        "  encoder,forward_decoder,backward_decoder,encoder_optimizer,forward_decoder_optimizer,backward_decoder_optimizer,epoch, train_losses, val_losses, files_suffix = load_model()\n",
        "  print(\"Training from epoch: \",epoch)\n",
        "else:\n",
        "  epoch = 1\n",
        "\n",
        "\n",
        "f = open(results_dir+'Train_loss_'+files_suffix+'.txt', 'w+')\n",
        "f_2 = open(results_dir+'Validation_loss_'+files_suffix+'.txt', 'w+')\n",
        "\n",
        "while epoch < n_epochs + 1:\n",
        "    \n",
        "    # Get training data for this cycle\n",
        "    for i in range(len(X_train)):\n",
        "      pair = (X_train[i],y_train[i])\n",
        "      \n",
        "      # Finding the complex word x_c and setting simple word y_s    \n",
        "      simplified_word, org_index , target_index = get_simple_word(pair[0],pair[1]) \n",
        "\n",
        "#       input_tensor = torch.tensor([SOS_TOKEN]+(input_vocab.index_sentence(pair[0], write=False))+[EOS_TOKEN]).cuda() # with EOS in the end\n",
        "#       backward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index-1::-1])+[SOS_TOKEN]).cuda() # from y_s-1 to SOS\n",
        "#       forward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index+1:])+[EOS_TOKEN]).cuda() # from y_s+1 to EOS\n",
        "\n",
        "      simplified_word = get_embeddings(simplified_word)\n",
        "      input_tensor =  torch.cat((get_embeddings('<SOS>'), torch.cat((get_sentence_embeddings(pair[0]),get_embeddings('<EOS>'))))).cuda()\n",
        "      backward_target_tensor = torch.cat((reverse_and_slice_tensor(get_sentence_embeddings(pair[1]), target_index), get_embeddings('<SOS>'))).cuda() # from y_s-1 to SOS\n",
        "      forward_target_tensor = torch.cat((get_sentence_embeddings(pair[1])[target_index+1:],get_embeddings('<EOS>'))).cuda() # from y_s+1 to EOS\n",
        "\n",
        "      # Run the train function\n",
        "      loss = train(input_tensor, backward_target_tensor, forward_target_tensor, simplified_word, forward_decoder, backward_decoder,\n",
        "                   encoder_optimizer, forward_decoder_optimizer, backward_decoder_optimizer, criterion,teacher_forcing_ratio)\n",
        "            \n",
        "      \n",
        "      # Keep track of loss\n",
        "      print_loss_total += loss\n",
        "      \n",
        "      if i == 0: continue\n",
        "\n",
        "      if i % 100 == 0:\n",
        "          print_loss_avg = print_loss_total / i\n",
        "          print('(Epoch %d)(pair number %d) %.4f' % (epoch,i, print_loss_avg))\n",
        "\n",
        "    if epoch == 0: continue\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        # Estimate FKBELU\n",
        "        fk_belu = 0\n",
        "#         idx = random.sample(range(len(X_train)), min(1000,len(X_train))) # get 1000 random indexes\n",
        "#         for i in idx: \n",
        "#           pair = (X_train[i],y_train[i])\n",
        "#           fk_belu += FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], evaluate(pair[0])[1:-1])\n",
        "#         fk_belu /= min(1000,len(X_train))\n",
        "        \n",
        "        # Train loss\n",
        "        print_loss_avg = print_loss_total / (print_every*len(X_train))\n",
        "        print_loss_total = 0\n",
        "        print('(Epoch %d) Train loss %.4f \\n' % (epoch, print_loss_avg))\n",
        "        f.write(\"(Epoch %d) Train loss %.4f \\n\" % (epoch, print_loss_avg))    \n",
        "        train_losses.append(print_loss_avg) \n",
        "        fk_belu_train.append(fk_belu)\n",
        "        \n",
        "        # Validation loss\n",
        "        val_loss, fk_belu = evaluate_val_loss(backward_decoder, forward_decoder,teacher_forcing_ratio=0)\n",
        "        print('(Epoch %d) Validation loss %.4f \\n' % (epoch, val_loss))\n",
        "        f_2.write(\"(Epoch %d) %.4f \\n\" % (epoch, val_loss))\n",
        "        val_losses.append(val_loss)\n",
        "        fk_belu_val.append(fk_belu)\n",
        "    epoch += 1\n",
        "    save_model(epoch)\n",
        "        \n",
        "print(\"Done!\")\n",
        "\n",
        "f.close()\n",
        "f_2.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "(Epoch 1)(pair number 100) 0.5805\n",
            "(Epoch 1)(pair number 200) 0.5513\n",
            "(Epoch 1)(pair number 300) 0.5286\n",
            "(Epoch 1)(pair number 400) 0.5050\n",
            "(Epoch 1)(pair number 500) 0.4889\n",
            "(Epoch 1)(pair number 600) 0.4786\n",
            "(Epoch 1)(pair number 700) 0.4757\n",
            "(Epoch 1)(pair number 800) 0.4688\n",
            "(Epoch 1)(pair number 900) 0.4612\n",
            "(Epoch 1) Train loss 0.4595 \n",
            "\n",
            "(Epoch 1) Validation loss 0.3995 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 2\n",
            "(Epoch 2)(pair number 100) 0.3601\n",
            "(Epoch 2)(pair number 200) 0.3727\n",
            "(Epoch 2)(pair number 300) 0.3790\n",
            "(Epoch 2)(pair number 400) 0.3786\n",
            "(Epoch 2)(pair number 500) 0.3827\n",
            "(Epoch 2)(pair number 600) 0.3800\n",
            "(Epoch 2)(pair number 700) 0.3844\n",
            "(Epoch 2)(pair number 800) 0.3841\n",
            "(Epoch 2)(pair number 900) 0.3848\n",
            "(Epoch 2) Train loss 0.3833 \n",
            "\n",
            "(Epoch 2) Validation loss 0.4343 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 3\n",
            "(Epoch 3)(pair number 100) 0.4064\n",
            "(Epoch 3)(pair number 200) 0.4009\n",
            "(Epoch 3)(pair number 300) 0.3960\n",
            "(Epoch 3)(pair number 400) 0.3919\n",
            "(Epoch 3)(pair number 500) 0.3937\n",
            "(Epoch 3)(pair number 600) 0.3898\n",
            "(Epoch 3)(pair number 700) 0.3891\n",
            "(Epoch 3)(pair number 800) 0.3885\n",
            "(Epoch 3)(pair number 900) 0.3870\n",
            "(Epoch 3) Train loss 0.3869 \n",
            "\n",
            "(Epoch 3) Validation loss 0.3612 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 4\n",
            "(Epoch 4)(pair number 100) 0.3638\n",
            "(Epoch 4)(pair number 200) 0.3474\n",
            "(Epoch 4)(pair number 300) 0.3549\n",
            "(Epoch 4)(pair number 400) 0.3600\n",
            "(Epoch 4)(pair number 500) 0.3597\n",
            "(Epoch 4)(pair number 600) 0.3611\n",
            "(Epoch 4)(pair number 700) 0.3640\n",
            "(Epoch 4)(pair number 800) 0.3628\n",
            "(Epoch 4)(pair number 900) 0.3601\n",
            "(Epoch 4) Train loss 0.3584 \n",
            "\n",
            "(Epoch 4) Validation loss 0.3283 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 5\n",
            "(Epoch 5)(pair number 100) 0.3375\n",
            "(Epoch 5)(pair number 200) 0.3380\n",
            "(Epoch 5)(pair number 300) 0.3343\n",
            "(Epoch 5)(pair number 400) 0.3356\n",
            "(Epoch 5)(pair number 500) 0.3421\n",
            "(Epoch 5)(pair number 600) 0.3444\n",
            "(Epoch 5)(pair number 700) 0.3462\n",
            "(Epoch 5)(pair number 800) 0.3447\n",
            "(Epoch 5)(pair number 900) 0.3438\n",
            "(Epoch 5) Train loss 0.3435 \n",
            "\n",
            "(Epoch 5) Validation loss 0.3679 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 6\n",
            "(Epoch 6)(pair number 100) 0.3129\n",
            "(Epoch 6)(pair number 200) 0.3306\n",
            "(Epoch 6)(pair number 300) 0.3414\n",
            "(Epoch 6)(pair number 400) 0.3440\n",
            "(Epoch 6)(pair number 500) 0.3399\n",
            "(Epoch 6)(pair number 600) 0.3458\n",
            "(Epoch 6)(pair number 700) 0.3463\n",
            "(Epoch 6)(pair number 800) 0.3453\n",
            "(Epoch 6)(pair number 900) 0.3418\n",
            "(Epoch 6) Train loss 0.3410 \n",
            "\n",
            "(Epoch 6) Validation loss 0.3511 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 7\n",
            "(Epoch 7)(pair number 100) 0.3121\n",
            "(Epoch 7)(pair number 200) 0.3279\n",
            "(Epoch 7)(pair number 300) 0.3318\n",
            "(Epoch 7)(pair number 400) 0.3250\n",
            "(Epoch 7)(pair number 500) 0.3251\n",
            "(Epoch 7)(pair number 600) 0.3252\n",
            "(Epoch 7)(pair number 700) 0.3269\n",
            "(Epoch 7)(pair number 800) 0.3251\n",
            "(Epoch 7)(pair number 900) 0.3236\n",
            "(Epoch 7) Train loss 0.3235 \n",
            "\n",
            "(Epoch 7) Validation loss 0.2937 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 8\n",
            "(Epoch 8)(pair number 100) 0.2987\n",
            "(Epoch 8)(pair number 200) 0.2989\n",
            "(Epoch 8)(pair number 300) 0.3028\n",
            "(Epoch 8)(pair number 400) 0.3077\n",
            "(Epoch 8)(pair number 500) 0.3067\n",
            "(Epoch 8)(pair number 600) 0.3099\n",
            "(Epoch 8)(pair number 700) 0.3124\n",
            "(Epoch 8)(pair number 800) 0.3142\n",
            "(Epoch 8)(pair number 900) 0.3155\n",
            "(Epoch 8) Train loss 0.3145 \n",
            "\n",
            "(Epoch 8) Validation loss 0.3140 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 9\n",
            "(Epoch 9)(pair number 100) 0.3157\n",
            "(Epoch 9)(pair number 200) 0.3272\n",
            "(Epoch 9)(pair number 300) 0.3227\n",
            "(Epoch 9)(pair number 400) 0.3232\n",
            "(Epoch 9)(pair number 500) 0.3181\n",
            "(Epoch 9)(pair number 600) 0.3142\n",
            "(Epoch 9)(pair number 700) 0.3162\n",
            "(Epoch 9)(pair number 800) 0.3186\n",
            "(Epoch 9)(pair number 900) 0.3196\n",
            "(Epoch 9) Train loss 0.3189 \n",
            "\n",
            "(Epoch 9) Validation loss 0.3368 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 10\n",
            "(Epoch 10)(pair number 100) 0.3207\n",
            "(Epoch 10)(pair number 200) 0.3202\n",
            "(Epoch 10)(pair number 300) 0.3186\n",
            "(Epoch 10)(pair number 400) 0.3197\n",
            "(Epoch 10)(pair number 500) 0.3169\n",
            "(Epoch 10)(pair number 600) 0.3165\n",
            "(Epoch 10)(pair number 700) 0.3180\n",
            "(Epoch 10)(pair number 800) 0.3159\n",
            "(Epoch 10)(pair number 900) 0.3137\n",
            "(Epoch 10) Train loss 0.3128 \n",
            "\n",
            "(Epoch 10) Validation loss 0.2938 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 11\n",
            "(Epoch 11)(pair number 100) 0.2805\n",
            "(Epoch 11)(pair number 200) 0.2962\n",
            "(Epoch 11)(pair number 300) 0.2988\n",
            "(Epoch 11)(pair number 400) 0.2960\n",
            "(Epoch 11)(pair number 500) 0.2934\n",
            "(Epoch 11)(pair number 600) 0.3002\n",
            "(Epoch 11)(pair number 700) 0.3019\n",
            "(Epoch 11)(pair number 800) 0.2991\n",
            "(Epoch 11)(pair number 900) 0.2970\n",
            "(Epoch 11) Train loss 0.2961 \n",
            "\n",
            "(Epoch 11) Validation loss 0.3156 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 12\n",
            "(Epoch 12)(pair number 100) 0.2956\n",
            "(Epoch 12)(pair number 200) 0.2924\n",
            "(Epoch 12)(pair number 300) 0.2977\n",
            "(Epoch 12)(pair number 400) 0.3032\n",
            "(Epoch 12)(pair number 500) 0.2973\n",
            "(Epoch 12)(pair number 600) 0.2987\n",
            "(Epoch 12)(pair number 700) 0.3031\n",
            "(Epoch 12)(pair number 800) 0.3024\n",
            "(Epoch 12)(pair number 900) 0.2970\n",
            "(Epoch 12) Train loss 0.2956 \n",
            "\n",
            "(Epoch 12) Validation loss 0.3106 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 13\n",
            "(Epoch 13)(pair number 100) 0.2861\n",
            "(Epoch 13)(pair number 200) 0.2723\n",
            "(Epoch 13)(pair number 300) 0.2755\n",
            "(Epoch 13)(pair number 400) 0.2781\n",
            "(Epoch 13)(pair number 500) 0.2764\n",
            "(Epoch 13)(pair number 600) 0.2781\n",
            "(Epoch 13)(pair number 700) 0.2831\n",
            "(Epoch 13)(pair number 800) 0.2849\n",
            "(Epoch 13)(pair number 900) 0.2854\n",
            "(Epoch 13) Train loss 0.2845 \n",
            "\n",
            "(Epoch 13) Validation loss 0.2833 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 14\n",
            "(Epoch 14)(pair number 100) 0.2804\n",
            "(Epoch 14)(pair number 200) 0.2876\n",
            "(Epoch 14)(pair number 300) 0.2904\n",
            "(Epoch 14)(pair number 400) 0.2923\n",
            "(Epoch 14)(pair number 500) 0.2833\n",
            "(Epoch 14)(pair number 600) 0.2855\n",
            "(Epoch 14)(pair number 700) 0.2848\n",
            "(Epoch 14)(pair number 800) 0.2846\n",
            "(Epoch 14)(pair number 900) 0.2857\n",
            "(Epoch 14) Train loss 0.2846 \n",
            "\n",
            "(Epoch 14) Validation loss 0.3114 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 15\n",
            "(Epoch 15)(pair number 100) 0.2737\n",
            "(Epoch 15)(pair number 200) 0.2852\n",
            "(Epoch 15)(pair number 300) 0.2778\n",
            "(Epoch 15)(pair number 400) 0.2842\n",
            "(Epoch 15)(pair number 500) 0.2810\n",
            "(Epoch 15)(pair number 600) 0.2800\n",
            "(Epoch 15)(pair number 700) 0.2817\n",
            "(Epoch 15)(pair number 800) 0.2816\n",
            "(Epoch 15)(pair number 900) 0.2806\n",
            "(Epoch 15) Train loss 0.2794 \n",
            "\n",
            "(Epoch 15) Validation loss 0.2737 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 16\n",
            "(Epoch 16)(pair number 100) 0.2646\n",
            "(Epoch 16)(pair number 200) 0.2646\n",
            "(Epoch 16)(pair number 300) 0.2658\n",
            "(Epoch 16)(pair number 400) 0.2689\n",
            "(Epoch 16)(pair number 500) 0.2683\n",
            "(Epoch 16)(pair number 600) 0.2674\n",
            "(Epoch 16)(pair number 700) 0.2722\n",
            "(Epoch 16)(pair number 800) 0.2701\n",
            "(Epoch 16)(pair number 900) 0.2713\n",
            "(Epoch 16) Train loss 0.2702 \n",
            "\n",
            "(Epoch 16) Validation loss 0.3246 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 17\n",
            "(Epoch 17)(pair number 100) 0.2474\n",
            "(Epoch 17)(pair number 200) 0.2654\n",
            "(Epoch 17)(pair number 300) 0.2678\n",
            "(Epoch 17)(pair number 400) 0.2712\n",
            "(Epoch 17)(pair number 500) 0.2666\n",
            "(Epoch 17)(pair number 600) 0.2687\n",
            "(Epoch 17)(pair number 700) 0.2728\n",
            "(Epoch 17)(pair number 800) 0.2697\n",
            "(Epoch 17)(pair number 900) 0.2686\n",
            "(Epoch 17) Train loss 0.2676 \n",
            "\n",
            "(Epoch 17) Validation loss 0.2981 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 18\n",
            "(Epoch 18)(pair number 100) 0.2559\n",
            "(Epoch 18)(pair number 200) 0.2607\n",
            "(Epoch 18)(pair number 300) 0.2603\n",
            "(Epoch 18)(pair number 400) 0.2554\n",
            "(Epoch 18)(pair number 500) 0.2521\n",
            "(Epoch 18)(pair number 600) 0.2540\n",
            "(Epoch 18)(pair number 700) 0.2584\n",
            "(Epoch 18)(pair number 800) 0.2592\n",
            "(Epoch 18)(pair number 900) 0.2589\n",
            "(Epoch 18) Train loss 0.2584 \n",
            "\n",
            "(Epoch 18) Validation loss 0.2938 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 19\n",
            "(Epoch 19)(pair number 100) 0.2497\n",
            "(Epoch 19)(pair number 200) 0.2617\n",
            "(Epoch 19)(pair number 300) 0.2617\n",
            "(Epoch 19)(pair number 400) 0.2566\n",
            "(Epoch 19)(pair number 500) 0.2537\n",
            "(Epoch 19)(pair number 600) 0.2518\n",
            "(Epoch 19)(pair number 700) 0.2546\n",
            "(Epoch 19)(pair number 800) 0.2555\n",
            "(Epoch 19)(pair number 900) 0.2542\n",
            "(Epoch 19) Train loss 0.2536 \n",
            "\n",
            "(Epoch 19) Validation loss 0.3032 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 20\n",
            "(Epoch 20)(pair number 100) 0.2695\n",
            "(Epoch 20)(pair number 200) 0.2609\n",
            "(Epoch 20)(pair number 300) 0.2618\n",
            "(Epoch 20)(pair number 400) 0.2618\n",
            "(Epoch 20)(pair number 500) 0.2577\n",
            "(Epoch 20)(pair number 600) 0.2564\n",
            "(Epoch 20)(pair number 700) 0.2584\n",
            "(Epoch 20)(pair number 800) 0.2588\n",
            "(Epoch 20)(pair number 900) 0.2568\n",
            "(Epoch 20) Train loss 0.2560 \n",
            "\n",
            "(Epoch 20) Validation loss 0.2782 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 21\n",
            "(Epoch 21)(pair number 100) 0.2510\n",
            "(Epoch 21)(pair number 200) 0.2648\n",
            "(Epoch 21)(pair number 300) 0.2598\n",
            "(Epoch 21)(pair number 400) 0.2483\n",
            "(Epoch 21)(pair number 500) 0.2446\n",
            "(Epoch 21)(pair number 600) 0.2461\n",
            "(Epoch 21)(pair number 700) 0.2472\n",
            "(Epoch 21)(pair number 800) 0.2445\n",
            "(Epoch 21)(pair number 900) 0.2452\n",
            "(Epoch 21) Train loss 0.2447 \n",
            "\n",
            "(Epoch 21) Validation loss 0.2686 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 22\n",
            "(Epoch 22)(pair number 100) 0.2335\n",
            "(Epoch 22)(pair number 200) 0.2416\n",
            "(Epoch 22)(pair number 300) 0.2414\n",
            "(Epoch 22)(pair number 400) 0.2438\n",
            "(Epoch 22)(pair number 500) 0.2452\n",
            "(Epoch 22)(pair number 600) 0.2443\n",
            "(Epoch 22)(pair number 700) 0.2445\n",
            "(Epoch 22)(pair number 800) 0.2464\n",
            "(Epoch 22)(pair number 900) 0.2450\n",
            "(Epoch 22) Train loss 0.2442 \n",
            "\n",
            "(Epoch 22) Validation loss 0.3046 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 23\n",
            "(Epoch 23)(pair number 100) 0.2392\n",
            "(Epoch 23)(pair number 200) 0.2461\n",
            "(Epoch 23)(pair number 300) 0.2446\n",
            "(Epoch 23)(pair number 400) 0.2458\n",
            "(Epoch 23)(pair number 500) 0.2419\n",
            "(Epoch 23)(pair number 600) 0.2381\n",
            "(Epoch 23)(pair number 700) 0.2388\n",
            "(Epoch 23)(pair number 800) 0.2388\n",
            "(Epoch 23)(pair number 900) 0.2342\n",
            "(Epoch 23) Train loss 0.2342 \n",
            "\n",
            "(Epoch 23) Validation loss 0.2948 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 24\n",
            "(Epoch 24)(pair number 100) 0.2368\n",
            "(Epoch 24)(pair number 200) 0.2463\n",
            "(Epoch 24)(pair number 300) 0.2442\n",
            "(Epoch 24)(pair number 400) 0.2480\n",
            "(Epoch 24)(pair number 500) 0.2441\n",
            "(Epoch 24)(pair number 600) 0.2416\n",
            "(Epoch 24)(pair number 700) 0.2411\n",
            "(Epoch 24)(pair number 800) 0.2416\n",
            "(Epoch 24)(pair number 900) 0.2397\n",
            "(Epoch 24) Train loss 0.2391 \n",
            "\n",
            "(Epoch 24) Validation loss 0.2842 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_pre_trained_n_epochs_30_LM_11.tar for epoch: 25\n",
            "(Epoch 25)(pair number 100) 0.2184\n",
            "(Epoch 25)(pair number 200) 0.2231\n",
            "(Epoch 25)(pair number 300) 0.2201\n",
            "(Epoch 25)(pair number 400) 0.2212\n",
            "(Epoch 25)(pair number 500) 0.2198\n",
            "(Epoch 25)(pair number 600) 0.2201\n",
            "(Epoch 25)(pair number 700) 0.2236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Ww1Fr4nTbJ",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xeTa0Ydor7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(5):\n",
        "    evaluate_randomly()\n",
        "    print('\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWvLBjfb2irk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(5):\n",
        "    evaluate_randomly_train()\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLPQAHIEiq62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write all evaluation results on validation data to file\n",
        "evaluate_validation()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K-OZGbGFlLY",
        "colab_type": "text"
      },
      "source": [
        "#### Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhi8FUrmFfEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plotting Losses\n",
        "file_name = results_dir+'Losses'+files_suffix+'.png'\n",
        "plotter = Plotter(train_losses,\"Train\", val_losses, \"Validation\")\n",
        "plotter.plot(file_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-YdCpwq_SSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Plotting FK_Belu\n",
        "# file_name = 'FK_Belu'+files_suffix+'.png'\n",
        "# plotter = Plotter(fk_belu_train,\"Train\", fk_belu_val, \"Validation\")\n",
        "# plotter.plot(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q7fPaYgS7bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
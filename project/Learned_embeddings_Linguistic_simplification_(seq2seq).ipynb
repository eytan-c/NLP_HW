{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Forward decoder hidden size Learned_embeddings_Linguistic_simplification_(seq2seq).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "5hOw9LpClYvs",
        "jXqSuxFclhJT"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob9S213blRMl",
        "colab_type": "text"
      },
      "source": [
        "###Imports and parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0sARhC6egnt",
        "colab_type": "code",
        "outputId": "aa7d9948-eaa5-4e23-be50-aa0656aaf568",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "working_dir = '/content/drive/My Drive/Colab Notebooks/nlp/data/'\n",
        "data_path = working_dir+'sim_dataset_23082019.csv'\n",
        "lm_data_path = working_dir+'he_htb-ud-train.txt'\n",
        "eng_wiki_normal_path = working_dir+'normal.aligned'\n",
        "eng_wiki_simple_path = working_dir+'simple.aligned'\n",
        "results_dir = working_dir+'Results/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZncvdMieiQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy\n",
        "from torch import autograd, nn, optim\n",
        "import torch.nn.functional as F\n",
        "import re\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "import datetime\n",
        "#import matplotlib # must be called here for plotting in nova\n",
        "#matplotlib.use('pdf') # must be called here for plotting in nova\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import json\n",
        "from nltk.translate import bleu_score\n",
        "from nltk.corpus import cmudict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8yHIdzzcJ1u",
        "colab_type": "text"
      },
      "source": [
        "#### Parameters definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6_ZKLPocJOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hidden_size = 500\n",
        "embedding_size = 200\n",
        "n_layers = 2\n",
        "dropout_p = 0.05\n",
        "teacher_forcing_ratio = 1\n",
        "n_epochs = 32\n",
        "learning_rate = 0.0003\n",
        "\n",
        "print_every = 1\n",
        "print_loss_total = 0 # Reset every print_every\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "fk_belu_train = []\n",
        "fk_belu_val = []\n",
        "dic_reg_to_sim = {}\n",
        " \n",
        "\n",
        "train_LM = False\n",
        "train_LM_n_epochs = 30\n",
        "train_LM_teacher_forcing_ratio = 0.7\n",
        "\n",
        "train_LM_data_size = 6000#00 # 6000 is everything\n",
        "data_size = 6000 # 121987 is everything in eng wiki\n",
        "entires_1_2 = True # use only entires type 1 and 2\n",
        "load_from_pickle = False\n",
        "eng_wiki = False\n",
        "load_model_flag = False\n",
        "model_params_file = working_dir+\"model_params_n_epochs_\"+str(n_epochs)+\".tar\"\n",
        "if eng_wiki:\n",
        "  json_dir = working_dir+'SPPDB_lexicon.json'\n",
        "else:\n",
        "  json_dir = working_dir+'lexicon.json'\n",
        "\n",
        "\n",
        "time = '{date:%Y-%m-%d_%H:%M:%S}'.format( date=datetime.datetime.now() ) \n",
        "if train_LM:\n",
        "  files_suffix = '_epoch_%d_LM_epoch_%d_time_%s' %(n_epochs,train_LM_n_epochs,time)\n",
        "elif eng_wiki:\n",
        "  files_suffix = '_English_wiki_epoch_%d_time_%s' % (n_epochs,time)\n",
        "else:\n",
        "  files_suffix = '_epoch_%d_time_%s' % (n_epochs,time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hOw9LpClYvs",
        "colab_type": "text"
      },
      "source": [
        "###Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66sASARZOZML",
        "colab_type": "text"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yKNnPMUdorH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "UNKNOWN = 2\n",
        "MAX_LENGTH = 30\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.index2word = {UNKNOWN:'__unk__'}\n",
        "        self.n_words = 3\n",
        "    \n",
        "    def index_sentence(self, sentence, write=True):\n",
        "        indexes = []\n",
        "        for w in sentence.strip().split(' '):\n",
        "            indexes.append(self.index_word(w, write))\n",
        "        return indexes\n",
        "            \n",
        "    def index_word(self, word, write=True):        \n",
        "        if word not in self.word2index:\n",
        "          if write:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words = self.n_words + 1\n",
        "          else:\n",
        "            return UNKNOWN\n",
        "        return self.word2index[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W74f7LHuO1Vu",
        "colab_type": "text"
      },
      "source": [
        "#### Read and organize sentances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nh0eJnNdorJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_string(s,only_heb=False):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    if only_heb:\n",
        "      s = re.sub(r\"[\\u0590-\\u05CF]+\", \"\", s)\n",
        "      s = re.sub(r\"[^א-ת.!?]+\", r\" \", s)\n",
        "      s = re.sub(r\"(^|\\s)\\\"(\\w)\", r\"\\1\\2\", re.sub(r\"(\\w)\\\"(\\s|$)\", r\"\\1\\2\", s))\n",
        "    else:\n",
        "      s = re.sub(r\"[^a-zA-Zא-ת.!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "  \n",
        "def filter_pair(p):\n",
        "    not_too_long = len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "    not_too_short = len(p[0].split(' ')) > 1 and len(p[1].split(' ')) > 1\n",
        "    return not_too_long and not_too_short\n",
        "\n",
        "def filter_pairs(pairs):\n",
        "    return [pair for pair in pairs if filter_pair(pair)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu_LEC2qdorN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_langs(lang1, lang2):    \n",
        "    if 'normal_heb' == lang1 or 'simple_heb' == lang2:\n",
        "      df = pandas.read_csv(data_path, error_bad_lines=False)#, encoding='utf-8')\n",
        "      if entires_1_2:\n",
        "        df = df.loc[df['entry_type']<3]\n",
        "      all_reg_sent = df['reg_sent']\n",
        "      all_sim_sent = df['sim_sent']\n",
        "\n",
        "      pairs = []\n",
        "      for i in list(df.index.values):\n",
        "        pairs.append([normalize_string(all_reg_sent[i],only_heb=True),normalize_string(all_sim_sent[i],only_heb=True)])\n",
        "        \n",
        "    if 'small_simple-wiki'==lang1 and 'small_normal-wiki'==lang2:\n",
        "      # Read the file and split into lines\n",
        "      lines = open('/content/drive/My Drive/Colab Notebooks/nlp/data/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
        "\n",
        "      # Split every line into pairs and normalize\n",
        "      lines = [line.split('\\t')[2] for line in lines]\n",
        "      pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
        "\n",
        "      pairs = [pairs[i]+pairs[i+1] for i in range(0,len(lines),2)]\n",
        "      \n",
        "    if 'normal-wiki'==lang1 and 'simple-wiki'==lang2:\n",
        "      # Read the files and split into lines\n",
        "      normal_lines = open(eng_wiki_normal_path).read().strip().split('\\n')\n",
        "      simple_lines = open(eng_wiki_simple_path).read().strip().split('\\n')\n",
        "      \n",
        "      # Split every line into pairs and normalize\n",
        "      normal_lines = [line.split('\\t')[2] for line in normal_lines]\n",
        "      simple_lines = [line.split('\\t')[2] for line in simple_lines]\n",
        "      \n",
        "      normal_lines = [[normalize_string(s) for s in l.split('\\t')] for l in normal_lines]\n",
        "      simple_lines = [[normalize_string(s) for s in l.split('\\t')] for l in simple_lines]\n",
        "\n",
        "      pairs = [normal_lines[i]+simple_lines[i] for i in range(len(normal_lines))]   \n",
        "      \n",
        "    if 'lm_train_heb'==lang1 and 'lm_train_heb'==lang2:\n",
        "      lines = open(lm_data_path).read().strip().replace('\\n', ' ').split('. ')\n",
        "      pairs = [[normalize_string(line,only_heb=True),normalize_string(line,only_heb=True)] for line in lines]\n",
        "        \n",
        "    return pairs\n",
        "\n",
        "\n",
        "def print_pair(p):\n",
        "    print(p[0])\n",
        "    print(p[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn-cAjHlS50c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairs_to_data(pairs, size):\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  for pair in pairs[:size]:\n",
        "    if pair[0]!=' ' and pair[1]!= ' ': # delete empty pairs\n",
        "      input_vocab.index_sentence(pair[0])\n",
        "      output_vocab.index_sentence(pair[1])\n",
        "      X.append(pair[0])\n",
        "      y.append(pair[1])\n",
        "  \n",
        "  print(\"Trimmed to %s non-empty sentence pairs\" % len(X))\n",
        "\n",
        "  # Print example\n",
        "  i = random.randint(0,len(X)-1)\n",
        "  print(X[i])\n",
        "  print(y[i])\n",
        "  return numpy.asarray(X), numpy.asarray(y)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCe7gl8hPDaq",
        "colab_type": "text"
      },
      "source": [
        "#### Init data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etOINPOMJoXu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_data(input_vocab,output_vocab,dic_reg_to_sim):\n",
        "  #input_vocab, output_vocab, pairs = prepare_data('simple-wiki', 'normal-wiki', True)\n",
        "  \n",
        "  print(\"Readin lexicon...\")\n",
        "  with open(json_dir,'r') as f: #,encoding='utf-8'\n",
        "\t   dic_reg_to_sim = json.load(f)\n",
        "  \n",
        "  print(\"Reading lines...\")\n",
        "  if train_LM:\n",
        "    #pairs = prepare_data('lm_train_heb', 'lm_train_heb')\n",
        "    pairs = read_langs('lm_train_heb', 'lm_train_heb')\n",
        "    print(\"Read %s sentence pairs for LM\" % len(pairs))    \n",
        "    pairs = filter_pairs(pairs)\n",
        "    X_lm, y_lm = pairs_to_data(pairs, train_LM_data_size)\n",
        "\n",
        "  if eng_wiki:\n",
        "    print(\"Data is in English from wikipedia\")\n",
        "    pairs = read_langs('normal-wiki', 'simple-wiki')\n",
        "  else:\n",
        "    print(\"Data is in Hebrew from our dataset\")\n",
        "    pairs = read_langs('normal_heb', 'simple_heb')\n",
        "  print(\"Read %s sentence pairs\" % len(pairs))    \n",
        "  pairs = filter_pairs(pairs)\n",
        "  X, y = pairs_to_data(pairs, data_size)\n",
        "  #dic_reg_to_sim = build_dic_reg_to_sim()\n",
        "  print(\"Sentences pairs left:%s\" % len(X))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=1)\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)\n",
        "\n",
        "  del X,y,pairs\n",
        "  \n",
        "  print(\"Train size:\",len(X_train))\n",
        "  print(\"Validation size:\",len(X_val))\n",
        "  print(\"Test size:\",len(X_test))\n",
        "\n",
        "  print(\"Data is ready!\")\n",
        "  if train_LM:\n",
        "    return input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm\n",
        "  else:\n",
        "    return input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXqSuxFclhJT",
        "colab_type": "text"
      },
      "source": [
        "###Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INj8_xGUllls",
        "colab_type": "text"
      },
      "source": [
        "####Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "836gR78kdorX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers        \n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.bi_grus = nn.GRU(input_size=embedding_size, hidden_size=hidden_size, num_layers=n_layers, batch_first=False, bidirectional=True)\n",
        "\n",
        "    def forward(self, word_inputs, hidden):\n",
        "        seq_len = len(word_inputs)\n",
        "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
        "             \n",
        "        output, hidden = self.bi_grus(embedded, hidden)\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self):\n",
        "        hidden = torch.zeros(self.n_layers*2, 1, self.hidden_size).cuda()\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NiUbmW7ltHN",
        "colab_type": "text"
      },
      "source": [
        "####Decoder with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnsZEQZPdora",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attn(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size \n",
        "        self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        seq_len = len(encoder_outputs)\n",
        "        attn_energies = torch.zeros(seq_len).cuda() # B x 1 x S\n",
        "        \n",
        "        for i in range(seq_len):\n",
        "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
        "\n",
        "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
        "        return F.softmax(attn_energies, 0).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    def score(self, hidden, encoder_output):        \n",
        "        energy = self.attn(encoder_output)        \n",
        "        energy = hidden.view(-1).dot(energy.view(-1))\n",
        "        return energy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0TR9N-Zdord",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding_size, output_size, n_layers=1, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        \n",
        "        # Keep parameters for reference\n",
        "        self.hidden_size = hidden_size * 2      \n",
        "        self.embedding_size = embedding_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "        \n",
        "        # Define layers\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(input_size=embedding_size+self.hidden_size, hidden_size=self.hidden_size, num_layers=n_layers, batch_first=False)\n",
        "        self.out = nn.Linear(self.hidden_size * 2, output_size)\n",
        "        self.attn = Attn(self.hidden_size)\n",
        "    \n",
        "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step at a time\n",
        "        \n",
        "        # Get the embedding of the current input word (last output word)\n",
        "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
        "        \n",
        "        # Combine embedded input word and last context, run through RNN\n",
        "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
        "        \n",
        "        #rnn_output, hidden = self.lstm(rnn_input, last_hidden)\n",
        "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
        "\n",
        "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
        "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
        "        \n",
        "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
        "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
        "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
        "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)), 1)\n",
        "        \n",
        "        # Return final output, hidden state\n",
        "        return output, context, hidden\n",
        "      \n",
        "    def init_hidden(self):\n",
        "      hidden = torch.zeros(self.n_layers, 1, int(self.hidden_size)).cuda()\n",
        "      return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWXWRDbm1t52",
        "colab_type": "text"
      },
      "source": [
        "####Complex words simplifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1G5nIm4c1sn9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # TODO\n",
        "# import random\n",
        "# def get_simple_word(org_sent, target_sent=None):\n",
        "#   if target_sent is None: # Test time\n",
        "#     org_index = random.randint(0,len(org_sent.split(' '))-1)\n",
        "#     return org_sent.split(' ')[org_index],org_index # simplified, index in org_sent\n",
        "#   else: # Train time\n",
        "#     org_index = random.randint(0,len(org_sent.split(' '))-1)\n",
        "#     target_index = random.randint(0,len(target_sent.split(' '))-1)\n",
        "#     return target_sent.split(' ')[target_index],org_index,target_index # simplified, index in org_sent, index in target_sent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwKduaMwqZa4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simplify(word, lexicon):\n",
        "\t\"\"\"\n",
        "\tAssumes that the word is in the lexicon.\n",
        "\tUses random.choice() since, if there are multiple simplifications, we want to\n",
        "\tpick randomly for them. If the word was simplified in the same way in\n",
        "\tmultiple entries, then the .choice() accounts for the number of time they\n",
        "\toccur.\n",
        "\t:param str word: word to simplify\n",
        "\t:param dict lexicon: dict to map word to simple version\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tsim = random.choice(lexicon[word])[1]\n",
        "\tif type(sim) == str:\n",
        "\t\tsim = [sim]\n",
        "\treturn \" \".join(sim)\n",
        "\n",
        "\n",
        "def get_simple_word_dict(org_sent, target_sent, lexicon, multi_word=0):\n",
        "\t\"\"\"\n",
        "\tDeal with no word in sentence: Pick random shared word. If non exists, pick random.\n",
        "\tDeal with multiple words in sentence: Pick first or Pick random. \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tRandom can be weighted or not.\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tDefault first (multi_word=0)\n",
        "\tIf multiple simplification - Random by weight - do be dealt with simplify() function\n",
        "\tNeed index in target sentence in Train. If simplification not in there, then pick random.\n",
        "\t:param org_sent:\n",
        "\t:param target_sent:\n",
        "\t:param lexicon:\n",
        "\t:param multi_word:\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tpotential = []\n",
        "\torg_sent_split = org_sent.split(' ')[:-1]\n",
        "\ttarget_sent_split = target_sent.split(' ')[:-1]\n",
        "\tfor i, word in enumerate(org_sent_split):\n",
        "\t\tif word in lexicon.keys():\n",
        "\t\t\tpotential.append((i, word))\n",
        "\tif len(potential) > 1:  # Multiple Potential Words to Simplify\n",
        "\t\tif multi_word == 0:  # Heuristic - Pick first word in sentence to simplify\n",
        "\t\t\tsimplified = simplify(potential[0][1], lexicon)  # returns a string (can be multiple words)\n",
        "\t\t\treg_index = potential[0][0]\n",
        "\t\telse:\n",
        "\t\t\t# Option 1 - Random choice with ints\n",
        "\t\t\t# rand_ind = random.randint(0, len(potential) - 1)\n",
        "\t\t\t# simplified = simplify(potential[rand_ind][1], lexicon)  # returns a string (can be multiple words)\n",
        "\t\t\t# reg_index = potential[rand_ind][0]\n",
        "\t\t\t# Option 2 - Random choice with choice\n",
        "\t\t\t# choice = random.choice(potential)\n",
        "\t\t\t# simplified = simplify(choice[1], lexicon)\n",
        "\t\t\t# reg_index = choice[0]\n",
        "\t\t\t# Option 3 - Random choice with choices (weighted\n",
        "\t\t\tws = [len(lexicon[w[1]]) for w in potential]\n",
        "\t\t\tchoice = random.choices(potential, ws)\n",
        "\t\t\tsimplified = simplify(choice[0][1], lexicon)\n",
        "\t\t\treg_index = choice[0][0]\n",
        "\telif len(potential) == 1:  # One potential word to simplify\n",
        "\t\tsimplified = simplify(potential[0][1], lexicon)  # returns a string (can be multiple words)\n",
        "\t\treg_index = potential[0][0]\n",
        "\telse:  # No potential words found\n",
        "\t\ttarget_words = set(target_sent_split) if target_sent is not None else set()\n",
        "\t\tshared_words = (set(org_sent_split) & target_words) - set('.')\n",
        "\t\tsimplified = random.choice(list(shared_words)) if shared_words != set() else random.choice(list(set(org_sent_split) - set('.')))\n",
        "\t\treg_index = org_sent_split.index(simplified)\n",
        "\tif target_sent is not None:  # Training time:\n",
        "\t\tif simplified in target_sent:  # If simplified in sim_sentence, then find index\n",
        "\t\t\tsim_index = target_sent_split.index(simplified.split(' ')[0])  # index of first word of simplified\n",
        "\t\telse:  # else pick random index\n",
        "\t\t\tsim_index = random.randint(0, len(target_sent_split) - 1)\n",
        "\t\treturn simplified, reg_index, sim_index  # simplified, index in org_sent, index in target_sent\n",
        "\telse:  # Test time:\n",
        "\t\treturn simplified, reg_index  # simplified, index in org_sent\n",
        "\n",
        "\n",
        "def get_simple_word_rand(org_sent, target_sent=None):\n",
        "\tif target_sent is None:  # Test time\n",
        "\t\torg_index = random.randint(0, len(org_sent.split(' '))-1)\n",
        "\t\treturn org_sent.split(' ')[org_index], org_index  # simplified, index in org_sent\n",
        "\telse:  # Train time\n",
        "\t\torg_index = random.randint(0, len(org_sent.split(' '))-1)\n",
        "\t\ttarget_index = random.randint(0, len(target_sent.split(' '))-1)\n",
        "\t\treturn target_sent.split(' ')[target_index], org_index, target_index  # simplified, index in org_sent, index in target_sent\n",
        "\n",
        "\n",
        "def get_simple_word(org_sent, target_sent=None, get_kind=0, lex=dic_reg_to_sim):\n",
        "\tassert type(get_kind) == int and get_kind < 4\n",
        "\tif get_kind == 0:\n",
        "\t\treturn get_simple_word_rand(org_sent, target_sent)\n",
        "\telif get_kind == 1:\n",
        "\t\treturn get_simple_word_dict(org_sent, target_sent, lexicon=lex)\n",
        "\telif get_kind == 2:\n",
        "\t\tpass\n",
        "\t\t# return get_simple_word_classifier(org_sent, target_sent=None, lexicon=lex)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17mWYtmVnI5O",
        "colab_type": "text"
      },
      "source": [
        "#### Save and Load models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5Ff-ZvenITy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(epoch):\n",
        "  torch.save({\n",
        "            'epoch': epoch,\n",
        "            'encoder_state_dict': encoder.state_dict(),\n",
        "            'forward_decoder_state_dict': forward_decoder.state_dict(),\n",
        "            'backward_decoder_state_dict': backward_decoder.state_dict(),\n",
        "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
        "            'forward_decoder_optimizer_state_dict': forward_decoder_optimizer.state_dict(),\n",
        "            'backward_decoder_optimizer_state_dict': backward_decoder_optimizer.state_dict(),     \n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'fk_belu_train': fk_belu_train,\n",
        "            'fk_belu_val': fk_belu_val,\n",
        "            'files_suffix': files_suffix,\n",
        "            }, working_dir+\"model_params_n_epochs_\"+str(n_epochs)+\".tar\")\n",
        "  print(\"Saved file:'\"+model_params_file+\" for epoch:\",str(epoch))\n",
        "\n",
        "\n",
        "def load_model():\n",
        "  device = torch.device(\"cuda\")\n",
        "#   encoder = EncoderRNN(input_vocab.n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "#   forward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "#   backward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "#   encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "#   forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate)\n",
        "#   backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "  checkpoint = torch.load(working_dir+\"model_params_n_epochs_\"+str(n_epochs)+\".tar\")\n",
        "  \n",
        "  encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
        "  forward_decoder.load_state_dict(checkpoint['forward_decoder_state_dict'])\n",
        "  backward_decoder.load_state_dict(checkpoint['backward_decoder_state_dict'])\n",
        "  \n",
        "  encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n",
        "  forward_decoder_optimizer.load_state_dict(checkpoint['forward_decoder_optimizer_state_dict'])\n",
        "  backward_decoder_optimizer.load_state_dict(checkpoint['backward_decoder_optimizer_state_dict'])\n",
        "  \n",
        "  epoch = checkpoint['epoch']\n",
        "  train_losses = checkpoint['train_losses']\n",
        "  val_losses = checkpoint['val_losses']\n",
        "  fk_belu_train = checkpoint['fk_belu_train']\n",
        "  fk_belu_val = checkpoint['fk_belu_val']\n",
        "  files_suffix = checkpoint['files_suffix']\n",
        "  \n",
        "  encoder.to(device)\n",
        "  forward_decoder.to(device)\n",
        "  backward_decoder.to(device)\n",
        "  encoder.train()\n",
        "  forward_decoder.train()\n",
        "  backward_decoder.train()\n",
        "  \n",
        "  return encoder,forward_decoder,backward_decoder,encoder_optimizer,forward_decoder_optimizer,backward_decoder_optimizer,epoch, train_losses, val_losses, fk_belu_train, fk_belu_val, files_suffix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpkz7V-Ppoc7",
        "colab_type": "text"
      },
      "source": [
        "### Train and Evaluation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1tqRoAmJHm",
        "colab_type": "text"
      },
      "source": [
        "####Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFL0an_dorj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_tensor, backward_target_tensor, forward_target_tensor, simplified_word, forward_decoder, backward_decoder,\n",
        "          encoder_optimizer, forward_decoder_optimizer, backward_decoder_optimizer, criterion, teacher_forcing_ratio=1, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients of both optimizers\n",
        "    encoder_optimizer.zero_grad()\n",
        "    backward_decoder_optimizer.zero_grad()\n",
        "    forward_decoder_optimizer.zero_grad()\n",
        "    loss = 0 # Added onto for each word\n",
        "    \n",
        "    # Get size of input and target sentences\n",
        "    input_length = input_tensor.size()[0]\n",
        "    backward_target_length = backward_target_tensor.size()[0]\n",
        "    forward_target_length = forward_target_tensor.size()[0]\n",
        "    \n",
        "    loss += run_model(input_tensor, backward_target_tensor, forward_target_tensor, input_length, backward_target_length, forward_target_length,\n",
        "                      simplified_word, backward_decoder, forward_decoder, criterion)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    backward_decoder_optimizer.step()\n",
        "    forward_decoder_optimizer.step()\n",
        "    \n",
        "    return loss.item() / (backward_target_length+forward_target_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um5KixbaJYMB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(input_tensor, backward_target_tensor, forward_target_tensor, input_length, backward_target_length, forward_target_length, simplified_word, backward_decoder,\n",
        "              forward_decoder, loss_func, teacher_forcing_ratio=teacher_forcing_ratio): \n",
        "  loss = 0 # Added onto for each word\n",
        "\n",
        "  # Run words through encoder\n",
        "  encoder_hidden = encoder.init_hidden()\n",
        "  \n",
        "  encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "  \n",
        "  # Prepare input and output variables\n",
        "  forward_decoder_input = torch.LongTensor([[output_vocab.index_word(simplified_word, write=False)]]).cuda() #forward pass from y_s\n",
        "  forward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "  backward_decoder_input = torch.LongTensor([[output_vocab.index_word(simplified_word, write=False)]]).cuda()\n",
        "  backward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "\n",
        "  backward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder\n",
        "  # change here#forward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder \n",
        "\n",
        "  # Run model on input\n",
        "\n",
        "  # Choose whether to use teacher forcing (Teacher forcing: Use the ground-truth target as the next input)\n",
        "  use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "  if use_teacher_forcing: \n",
        "      # Backward pass\n",
        "      for di in range(backward_target_length):\n",
        "          backward_decoder_output, backward_decoder_context, backward_decoder_hidden = backward_decoder(\n",
        "              backward_decoder_input, backward_decoder_context, backward_decoder_hidden, encoder_outputs)\n",
        "          loss += loss_func(backward_decoder_output[0].view(-1).unsqueeze(0), backward_target_tensor[di].unsqueeze(0))\n",
        "          backward_decoder_input = backward_target_tensor[di] # Next target is next input\n",
        "\n",
        "      # Forward pass\n",
        "      forward_decoder_hidden = backward_decoder_hidden # change here\n",
        "      for di in range(forward_target_length):\n",
        "          forward_decoder_output, forward_decoder_context, forward_decoder_hidden = forward_decoder(\n",
        "              forward_decoder_input, forward_decoder_context, forward_decoder_hidden, encoder_outputs)\n",
        "          loss += loss_func(forward_decoder_output[0].view(-1).unsqueeze(0), forward_target_tensor[di].unsqueeze(0))\n",
        "          forward_decoder_input = forward_target_tensor[di] # Next target is next input\n",
        "\n",
        "  else: # Without teacher forcing: use network's own prediction as the next input\n",
        "      # Backward pass\n",
        "      for di in range(backward_target_length):\n",
        "          backward_decoder_output, backward_decoder_context, backward_decoder_hidden = backward_decoder(\n",
        "              backward_decoder_input, backward_decoder_context, backward_decoder_hidden, encoder_outputs)\n",
        "          loss += loss_func(backward_decoder_output[0].view(-1).unsqueeze(0), backward_target_tensor[di].unsqueeze(0))\n",
        "\n",
        "          # Get most likely word index (highest value) from output\n",
        "          topv, topi = backward_decoder_output.topk(1)\n",
        "          ni = topi[0][0]\n",
        "\n",
        "          backward_decoder_input = torch.LongTensor([[ni]]).cuda() # Chosen word is next input\n",
        "\n",
        "          # Stop at start of sentence (not necessary when using known targets)\n",
        "          if ni == SOS_TOKEN: break\n",
        "\n",
        "      # Forward pass\n",
        "      forward_decoder_hidden = backward_decoder_hidden # change here\n",
        "      for di in range(forward_target_length):\n",
        "          forward_decoder_output, forward_decoder_context, forward_decoder_hidden = forward_decoder(\n",
        "              forward_decoder_input, forward_decoder_context, forward_decoder_hidden, encoder_outputs)\n",
        "          loss += loss_func(forward_decoder_output[0].view(-1).unsqueeze(0), forward_target_tensor[di].unsqueeze(0))\n",
        "\n",
        "          # Get most likely word index (highest value) from output\n",
        "          topv, topi = forward_decoder_output.topk(1)\n",
        "          ni = topi[0][0]\n",
        "\n",
        "          forward_decoder_input = torch.LongTensor([[ni]]).cuda() # Chosen word is next input\n",
        "\n",
        "          # Stop at end of sentence (not necessary when using known targets)\n",
        "          if ni == EOS_TOKEN: break\n",
        "    \n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeONurq3qD-e",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzV2YNg7SwDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_val_loss(backward_decoder, forward_decoder,teacher_forcing_ratio=0):\n",
        "    backward_decoder.eval()\n",
        "    forward_decoder.eval()\n",
        "    encoder.eval()\n",
        "    #cur_val_loss = 0\n",
        "    val_loss_sum = 0\n",
        "    fk_belu = 0\n",
        "    \n",
        "    for i in range(len(X_val)):    \n",
        "      pair = (X_val[i],y_val[i])         \n",
        "      # Finding the complex word x_c and setting simple word y_s    \n",
        "      simplified_word, org_index , target_index = get_simple_word(pair[0],pair[1]) \n",
        "      input_tensor = torch.tensor([SOS_TOKEN]+(input_vocab.index_sentence(pair[0], write=False))+[EOS_TOKEN]).cuda()\n",
        "      backward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index-1::-1])+[SOS_TOKEN]).cuda() # from y_s-1 to SOS\n",
        "      forward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index+1:])+[EOS_TOKEN]).cuda() # from y_s+1 to EOS\n",
        "      \n",
        "      # Get size of input and target sentences\n",
        "      input_length = input_tensor.size()[0]\n",
        "      backward_target_length = backward_target_tensor.size()[0]\n",
        "      forward_target_length = forward_target_tensor.size()[0]\n",
        "      \n",
        "      cur_val_loss = run_model(input_tensor, backward_target_tensor, forward_target_tensor, input_length, backward_target_length, forward_target_length,\n",
        "                               simplified_word, backward_decoder, forward_decoder, nn.NLLLoss())\n",
        "      val_loss_sum += cur_val_loss.item() / (backward_target_length+forward_target_length)\n",
        "      \n",
        "      fk_belu += FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], evaluate(pair[0])[1:-1])\n",
        "    \n",
        "    backward_decoder.train()\n",
        "    forward_decoder.train()\n",
        "    encoder.train()\n",
        "    \n",
        "    return val_loss_sum/len(X_val), fk_belu/len(X_val)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqBumV5TMyp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs, max_length=MAX_LENGTH, is_backward=None):\n",
        "    decoded_words = []\n",
        "    decoder_attentions = torch.zeros(max_length, max_length).cuda()\n",
        "    \n",
        "    # Run through decoder\n",
        "    for di in range(max_length):\n",
        "        if is_backward:\n",
        "          decoder_output, decoder_context, decoder_hidden = backward_decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
        "        else:\n",
        "          decoder_output, decoder_context, decoder_hidden = forward_decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
        "        # Choose top word from output\n",
        "        topv, topi = decoder_output.cpu().topk(1)\n",
        "        ni = topi[0][0].item()        \n",
        "        \n",
        "        if (ni == SOS_TOKEN and is_backward) or (ni == EOS_TOKEN and not is_backward):\n",
        "            if is_backward:\n",
        "              decoded_words.append('<SOS>')\n",
        "            else:\n",
        "              decoded_words.append('<EOS>')\n",
        "            break\n",
        "        else:\n",
        "            if ni == SOS_TOKEN:\n",
        "               decoded_words.append(\"<SOS?>\")\n",
        "            elif ni == EOS_TOKEN:\n",
        "               decoded_words.append(\"<EOS?>\")\n",
        "            else:\n",
        "               decoded_words.append(output_vocab.index2word[ni])\n",
        "            \n",
        "        # Next input is chosen word\n",
        "        decoder_input = torch.LongTensor([[ni]]).cuda()\n",
        "        \n",
        "    return decoded_words, decoder_hidden\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivPXrB-idor1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence, max_length=MAX_LENGTH):\n",
        "      \n",
        "    input_tensor = torch.tensor(input_vocab.index_sentence(sentence)).cuda()\n",
        "    input_length = input_tensor.size()[0]\n",
        "    \n",
        "    # Run through encoder\n",
        "    encoder_hidden = encoder.init_hidden()\n",
        "    encoder_outputs, encoder_hidden = encoder(input_tensor, encoder_hidden)\n",
        "    \n",
        "    # Finding the complex word x_c and setting simple word y_s    \n",
        "    simplified_word, org_index = get_simple_word(sentence) \n",
        "\n",
        "    # Create starting vectors for backward decoder\n",
        "    backward_decoder_input = torch.LongTensor([[output_vocab.index_word(simplified_word, write=False)]]).cuda() # y_s\n",
        "    backward_decoder_context = torch.zeros(1, backward_decoder.hidden_size).cuda()\n",
        "    backward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder\n",
        "    \n",
        "    # Create starting vectors for forward decoder\n",
        "    forward_decoder_input = torch.LongTensor([[output_vocab.index_word(simplified_word, write=False)]]).cuda() # SOS\n",
        "    forward_decoder_context = torch.zeros(1, forward_decoder.hidden_size).cuda()\n",
        "    #forward_decoder_hidden = encoder_hidden.view(-1,1,encoder_hidden.shape[2]*2) # Use last hidden state from encoder to start decoder\n",
        "    \n",
        "    # Run through decoders\n",
        "    backward_words, backward_decoder_hidden = run_decoder(backward_decoder_input, backward_decoder_context, backward_decoder_hidden, encoder_outputs, max_length, is_backward=True)\n",
        "    forward_words, _ = run_decoder(forward_decoder_input, forward_decoder_context, backward_decoder_hidden, encoder_outputs, max_length, is_backward=False)\n",
        "    \n",
        "    # Bulid sentance\n",
        "    simplified_word = simplified_word \n",
        "    decoded_words = backward_words[::-1]+[simplified_word]+forward_words\n",
        "    \n",
        "    return decoded_words # with no SOS and EOS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExP97mC2dor5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_randomly():\n",
        "    #pair = random.choice(pairs)\n",
        "    i = random.randint(0,len(X_val)-1)\n",
        "    pair = [X_val[i],y_val[i]]\n",
        "  \n",
        "    output_words = evaluate(pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    \n",
        "    print(\"Validation example\")\n",
        "    print('FKBLEU: ', FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], output_words[1:-1]))\n",
        "    print(pair[0])\n",
        "    print(pair[1])\n",
        "    print(output_sentence)\n",
        "    \n",
        "def evaluate_idx(idx):\n",
        "    #pair = random.choice(pairs)\n",
        "    for i in idx:\n",
        "      pair = [X_val[i],y_val[i]]\n",
        "\n",
        "      output_words = evaluate(pair[0])\n",
        "      output_sentence = ' '.join(output_words)\n",
        "\n",
        "      print(\"Validation example\")\n",
        "      print('FKBLEU: ', FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], output_words[1:-1]))\n",
        "      print(pair[0])\n",
        "      print(pair[1])\n",
        "      print(output_sentence[1:-1])\n",
        "      print('\\n')\n",
        "    \n",
        "    \n",
        "def evaluate_randomly_train():\n",
        "    #pair = random.choice(pairs)\n",
        "    i = random.randint(0,len(X_train)-1)\n",
        "    pair = [X_train[i],y_train[i]]\n",
        "  \n",
        "    output_words = evaluate(pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    \n",
        "    print(\"Train example\")\n",
        "    print('FKBLEU: ', FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], output_words[1:-1]))\n",
        "    print(pair[0])\n",
        "    print(pair[1])\n",
        "    print(output_sentence)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC7I-rNifzl8",
        "colab_type": "text"
      },
      "source": [
        "##### FKBELU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DixbcyuMfybg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iBLEU(input_sent, reference, candidate, alpha=0.9):\n",
        "\t\"\"\"\n",
        "\tCalculate iBLEU according to Xu et. al. 2016\n",
        "\t:param input_sent: original sentence\n",
        "\t:param reference: the target sentences to test by\n",
        "\t:param candidate: a proposed sentence from the input\n",
        "\t:param alpha: default param 0.9 from Sun and Zhou (2012)\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\tsmooth = bleu_score.SmoothingFunction()\n",
        "\tif len(candidate) < 2:\n",
        "\t\treturn 0.0\n",
        "\tref_candidate = bleu_score.sentence_bleu(reference, candidate, smoothing_function=smooth.method7)\n",
        "\tinput_candidate = bleu_score.sentence_bleu(input_sent, candidate, smoothing_function=smooth.method7)\n",
        "\t# print('ref_candidate:', '%s; ' % ref_candidate, 'input_candidate:','%s; ' % input_candidate)\n",
        "\t# print('iBLEU (alpha * ref_candidate - (1 - alpha) * input_candidate) =\\n %s' % (alpha * ref_candidate - (1 - alpha) * input_candidate))\n",
        "\treturn alpha * ref_candidate - (1 - alpha) * input_candidate\n",
        "\n",
        "\t\n",
        "\n",
        "def FK(text, language='heb'):\n",
        "\t\"\"\"\n",
        "\tTODO: count syllables not with heuristic\n",
        "\tCalculate Flesch-Kincaid Index (Kincaid et al 1975) according to Xu et. al. 2016\n",
        "\t:param language: Used for syllables count heuristic\n",
        "\t:param text: Assumes is a list of lists of words (list of sentences as lists of words)\n",
        "\t:return:\n",
        "\t\"\"\"\n",
        "\t# if isinstance(text, list) and all(isinstance(sen, list) for sen in text):\n",
        "\t# \tfor sen in text:\n",
        "\t# \t\tassert all(isinstance(w, str) for w in sen)\n",
        "\t\n",
        "\tnum_words = 0\n",
        "\tnum_sents = 0\n",
        "\tnum_syllables = 0\n",
        "\tif eng_wiki:#language == 'eng':\n",
        "\t\t#parser = pyphen.Pyphen('en_us')\n",
        "\t\ttry:\n",
        "\t\t\tparser = cmudict.dict()\n",
        "\t\texcept LookupError:\n",
        "\t\t\timport nltk\n",
        "\t\t\tnltk.download('cmudict')\n",
        "\t\t\tparser = cmudict.dict()\n",
        "\telse:\n",
        "\t\tparser = None\n",
        "\t# Gather numerical calculations\n",
        "\tfor sen in text:\n",
        "\t\tnum_sents += 1\n",
        "\t\tfor word in sen:\n",
        "\t\t\tnum_words += 1\n",
        "\t\t\tif language == 'heb':  # heuristic that each letter is a syllable in hebrew\n",
        "\t\t\t\tnum_syllables += len(word)\n",
        "\t\t\telif language == 'eng':  # syllable parser for English\n",
        "\t\t\t\tnum_syllables += len(parser.inserted(word).split('-'))\n",
        "\t# print('Words, Sents, Syllables: %s, %s, %s' % (num_words, num_sents, num_syllables))\n",
        "\treturn 0.39 * (num_words / num_sents) + 11.8 * (num_syllables / num_words) - 15.59\n",
        "\n",
        "\n",
        "def FKdiff(input_sent, candidate):\n",
        "  \"\"\"\n",
        "  \n",
        "  :param input_sent: Assumes input sent is a list of lists of words\n",
        "  :param candidate: Assumes candidate is a list of words\n",
        "  :return:\n",
        "  \"\"\"\n",
        "  # using torch\n",
        "  # print('FKdiff:', torch.sigmoid(FK([candidate]) - FK(input_sent)))\n",
        "  # return torch.nn.functional.sigmoid(FK([candidate]) - FK(input_sent))\n",
        "  # using python native\n",
        "  x = FK([candidate]) - FK(input_sent)\n",
        "  # print('FK(candidate): %s;  FK(input_sent): %s' % (FK([candidate]), FK(input_sent)))\n",
        "  # print('x: %s' % x)\n",
        "  # print('FKdiff:', 1 / (1 + numpy.exp(-x)))\n",
        "  return 1 / (1 + numpy.exp(-x))\n",
        "\n",
        "\n",
        "def FKBLEU(input_sent, references, candidate):\n",
        "\t\"\"\"\n",
        "\tCalculate iBLEU according to Xu et. al. 2016\n",
        "\t:param input_sent: original sentence. Assumes list of lists of words\n",
        "\t:param references: the target sentences to test by. Assumes list of lists of words\n",
        "\t:param candidate: a proposed sentence from the input. List of words.\n",
        "\t:return: (FK_BELU)\n",
        "\t\"\"\"\n",
        "\t#print('Input: %s, refrences: %s, candidate: %s' % (input_sent, references, candidate))\n",
        "\ti_belu = iBLEU(input_sent, references, candidate)\n",
        "\tfk_diff = FKdiff(input_sent, candidate)\n",
        "\treturn i_belu * fk_diff#, i_belu, FK([candidate])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiQAHpxEqxzW",
        "colab_type": "text"
      },
      "source": [
        "#### Create results files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg65Xlp2qi25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write results to file\n",
        "def evaluate_validation():\n",
        "  fk_belu = 0\n",
        "  i_belu = 0\n",
        "  fk = 0\n",
        "  f = open(results_dir+'Results'+files_suffix+'.txt', 'w+')\n",
        "  \n",
        "  for i in range(len(X_val)):\n",
        "    pair = [X_val[i],y_val[i]]\n",
        "    output_words = evaluate(pair[0])\n",
        "    output_sentence = ' '.join(output_words)\n",
        "    f.write(\"This is line %d\\r\\n\" % (i+1))\n",
        "    f.write(pair[0]+\" \\n\")\n",
        "    f.write(pair[1]+\" \\n\")\n",
        "    f.write(output_sentence+\" \\n\")\n",
        "    input_sent = [pair[0].split(' ')]\n",
        "    references = [pair[1].split(' ')]\n",
        "    candidate = output_words[1:-1]\n",
        "    fk_belu += FKBLEU(input_sent, references , candidate)\n",
        "    i_belu += iBLEU(input_sent, references, candidate)\n",
        "    fk += FK(candidate)\n",
        "  print(\"###########################################\")\n",
        "  print(\"\\n This is FK-BELU on the validation %.18f \\n\" % (fk_belu/len(X_val)))\n",
        "  print(\"\\n This is iBELU on the validation %.18f \\n\" % (i_belu/len(X_val)))\n",
        "  print(\"\\n This is FK on the validation %.18f \\n\" % (fk/len(X_val)))\n",
        "  f.write(\"\\n This is FK-BELU on the validation %.18f \\n\" % (fk_belu/len(X_val)))\n",
        "  f.write(\"\\n This is i_belu on the validation %.18f \\n\" % (i_belu/len(X_val)))\n",
        "  f.write(\"\\n This is FK on the validation %.18f \\n\" % (fk/len(X_val)))\n",
        "  f.write(\"\\n Those are the run parameters: LR %.5f, dropout %.5f, n_layers %d, embedding_size %d,hidden_size %d \\n\" % (learning_rate,dropout_p,n_layers,embedding_size,hidden_size))\n",
        "  print(\"Wrote those results to file\")\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5fSU8ZGq2Z9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Plotter:\n",
        "    def __init__(self, first_losses, fist_label, sec_losses, sec_label, title='Negative Log Likelihood Loss'):\n",
        "        self.first_losses = first_losses\n",
        "        self.fist_label = fist_label\n",
        "        self.sec_losses = sec_losses\n",
        "        self.sec_label = sec_label\n",
        "        self.title = title\n",
        "\n",
        "    def plot(self, file_name):\n",
        "        \"\"\"Plot the loss per epoch\"\"\"\n",
        "        line1, = plt.plot(range(len(self.first_losses)), self.first_losses, label=self.fist_label)\n",
        "        line2, = plt.plot(range(len(self.sec_losses)), self.sec_losses, label=self.sec_label)\n",
        "        plt.legend(handles=[line1,line2])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title(self.title)\n",
        "        plt.grid(True)\n",
        "        plt.savefig(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3nJX5Wclz_m",
        "colab_type": "text"
      },
      "source": [
        "###Running models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4uUKZcSmMa3",
        "colab_type": "text"
      },
      "source": [
        "####Models' init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sVCDg_3doro",
        "colab_type": "code",
        "outputId": "ce1d7fd4-6cc7-4373-dda3-52eb67b37cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Data Load\n",
        "\n",
        "try:\n",
        "  if not load_from_pickle:\n",
        "    raise IOError(\"Do not load from pickle!\")\n",
        "  if train_LM:\n",
        "    input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm = pickle.load(open(working_dir+\"learned_data.pickle\", \"rb\"))\n",
        "  else:\n",
        "    input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test = pickle.load(open(working_dir+\"learned_data.pickle\", \"rb\"))\n",
        "except (OSError, IOError) as e:\n",
        "  input_vocab = Vocab(\"Heb-reg\")\n",
        "  output_vocab = Vocab(\"Heb-simple\")\n",
        "  dic_reg_to_sim = {}\n",
        "  if train_LM:\n",
        "    input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm = init_data(input_vocab,output_vocab,dic_reg_to_sim)\n",
        "    pickle.dump([input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test, X_lm, y_lm], open(working_dir+\"learned_data.pickle\", \"wb\"))\n",
        "  else:\n",
        "    input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test = init_data(input_vocab,output_vocab,dic_reg_to_sim)\n",
        "    pickle.dump([input_vocab, output_vocab, dic_reg_to_sim, X_train, y_train, X_val, y_val, X_test, y_test], open(working_dir+\"learned_data.pickle\", \"wb\"))\n",
        "  \n",
        "# Initialize models\n",
        "encoder = EncoderRNN(input_vocab.n_words, embedding_size, hidden_size, n_layers).cuda()\n",
        "forward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "backward_decoder = AttnDecoderRNN(hidden_size, embedding_size, output_vocab.n_words, n_layers, dropout_p=dropout_p).cuda()\n",
        "\n",
        "encoder.train()\n",
        "forward_decoder.train()\n",
        "backward_decoder.train()\n",
        "\n",
        "# Initialize optimizers and criterion\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate)\n",
        "backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate)\n",
        "criterion = nn.NLLLoss()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Readin lexicon...\n",
            "Reading lines...\n",
            "Data is in Hebrew from our dataset\n",
            "Read 1406 sentence pairs\n",
            "Trimmed to 1244 non-empty sentence pairs\n",
            "כלבי נחייה\n",
            "כלבי נחייה\n",
            "Sentences pairs left:1244\n",
            "Train size: 1062\n",
            "Validation size: 119\n",
            "Test size: 63\n",
            "Data is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5hGCEk_l_Pb",
        "colab_type": "text"
      },
      "source": [
        "####Training as LM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "LcrHj6HSdort",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if train_LM:\n",
        "  print(\"Training model as LM...\")\n",
        "  \n",
        "  if load_model_flag:\n",
        "    print(\"Loading model's parameters in LM...\")\n",
        "    encoder,forward_decoder,backward_decoder,encoder_optimizer,forward_decoder_optimizer,backward_decoder_optimizer,epoch, train_losses, val_losses, fk_belu_train, fk_belu_val, files_suffix = load_model()\n",
        "    print(\"Training LM from epoch: \",epoch)\n",
        "  else:\n",
        "    epoch = 1\n",
        "  \n",
        "  f = open(results_dir+'Train_LM_loss_'+files_suffix+'.txt', 'w+')\n",
        "  \n",
        "  while epoch < train_LM_n_epochs + 1:\n",
        "      for i in range(len(X_lm)):\n",
        "        pair = (X_lm[i],y_lm[i])        \n",
        "        splitting_index = (len(pair[0].split(' '))-1)//2\n",
        "        simplified_word = output_vocab.index_word(pair[1].split(' ')[splitting_index])\n",
        "\n",
        "        input_tensor = torch.tensor([SOS_TOKEN]+(input_vocab.index_sentence(pair[0], write=False))+[EOS_TOKEN]).cuda()\n",
        "        backward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[splitting_index-1::-1])+[SOS_TOKEN]).cuda() # from y_s-1 to SOS\n",
        "        forward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[splitting_index+1:])+[EOS_TOKEN]).cuda() # from y_s+1 to EOS\n",
        "\n",
        "        # Run the train function\n",
        "        loss = train(input_tensor, backward_target_tensor, forward_target_tensor, simplified_word, forward_decoder, backward_decoder,\n",
        "                     encoder_optimizer, forward_decoder_optimizer, backward_decoder_optimizer, criterion,train_LM_teacher_forcing_ratio)\n",
        "\n",
        "        # Keep track of loss\n",
        "        print_loss_total += loss\n",
        "        if i == 0: continue\n",
        "          \n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print_loss_avg = print_loss_total / i\n",
        "            print('(Epoch %d)(pair number %d) %.4f' % (epoch,i, print_loss_avg))\n",
        "\n",
        "      if epoch == 0: continue\n",
        "\n",
        "      if epoch % print_every == 0:\n",
        "          print_loss_avg = print_loss_total / (print_every * len(X_lm))\n",
        "          print_loss_total = 0\n",
        "          print('(Epoch %d) %.4f' % (epoch, print_loss_avg))\n",
        "          f.write(\"(Epoch %d) %.4f \\n\" % (epoch, print_loss_avg))\n",
        "      epoch += 1\n",
        "      save_model(epoch)\n",
        "\n",
        "  print(\"Done!\")\n",
        "  f.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NLIU53J5kIV",
        "colab_type": "code",
        "outputId": "bd263d33-5b18-4776-de7f-5b846f1ccb8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(output_vocab.n_words)\n",
        "#import math\n",
        "#print(\"First loss should be:\",-math.log(1/output_vocab.n_words))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc0Lg9aPT6p3",
        "colab_type": "text"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7iqEVOWT5OM",
        "colab_type": "code",
        "outputId": "7460c525-0ff5-47e6-95d7-78314b403054",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Begin!\n",
        "\n",
        "print(\"Training...\")\n",
        "\n",
        "if load_model_flag and not train_LM:\n",
        "  print(\"Loading model's parameters...\")\n",
        "  encoder,forward_decoder,backward_decoder,encoder_optimizer,forward_decoder_optimizer,backward_decoder_optimizer,epoch, train_losses, val_losses, fk_belu_train, fk_belu_val, files_suffix = load_model()\n",
        "  print(\"Training from epoch: \",epoch)\n",
        "else:\n",
        "  epoch = 1\n",
        "\n",
        "f = open(results_dir+'Train_loss_'+files_suffix+'.txt', 'w+')\n",
        "f_2 = open(results_dir+'Validation_loss_'+files_suffix+'.txt', 'w+')\n",
        "\n",
        "while epoch < n_epochs + 1:\n",
        "    \n",
        "    # Get training data for this cycle\n",
        "    for i in range(len(X_train)):\n",
        "      pair = (X_train[i],y_train[i])\n",
        "      \n",
        "      # Finding the complex word x_c and setting simple word y_s    \n",
        "      simplified_word, org_index , target_index = get_simple_word(pair[0],pair[1]) \n",
        "\n",
        "      input_tensor = torch.tensor([SOS_TOKEN]+(input_vocab.index_sentence(pair[0], write=False))+[EOS_TOKEN]).cuda() # with EOS in the end\n",
        "      backward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index-1::-1])+[SOS_TOKEN]).cuda() # from y_s-1 to SOS\n",
        "\n",
        "      #forward_target_tensor = torch.tensor([SOS_TOKEN]+(output_vocab.index_sentence(pair[1], write=False))+[EOS_TOKEN]).cuda() # from SOS to EOS (use only y_s+1 to EOS)\n",
        "      forward_target_tensor = torch.tensor((output_vocab.index_sentence(pair[1], write=False)[target_index+1:])+[EOS_TOKEN]).cuda() # from y_s+1 to EOS\n",
        "\n",
        "      # Run the train function\n",
        "      loss = train(input_tensor, backward_target_tensor, forward_target_tensor, simplified_word, forward_decoder, backward_decoder,\n",
        "                   encoder_optimizer, forward_decoder_optimizer, backward_decoder_optimizer, criterion,teacher_forcing_ratio)\n",
        "            \n",
        "      \n",
        "      # Keep track of loss\n",
        "      print_loss_total += loss\n",
        "      \n",
        "      if i == 0: continue\n",
        "\n",
        "      if i % 100 == 0:\n",
        "          print_loss_avg = print_loss_total / i\n",
        "          print('(Epoch %d)(pair number %d) %.4f' % (epoch,i, print_loss_avg))\n",
        "\n",
        "    if epoch == 0: continue\n",
        "\n",
        "    if epoch % print_every == 0:\n",
        "        # Estimate FKBELU\n",
        "        fk_belu = 0\n",
        "        idx = random.sample(range(len(X_train)), min(1000,len(X_train))) # get 1000 random indexes\n",
        "        for i in idx: \n",
        "          pair = (X_train[i],y_train[i])\n",
        "          fk_belu += FKBLEU([pair[0].split(' ')], [pair[1].split(' ')], evaluate(pair[0])[1:-1])\n",
        "        fk_belu /= min(1000,len(X_train))\n",
        "        \n",
        "        # Train loss\n",
        "        print_loss_avg = print_loss_total / (print_every*len(X_train))\n",
        "        print_loss_total = 0\n",
        "        print('(Epoch %d) Train loss %.4f , fk_belu %.18f \\n' % (epoch, print_loss_avg, fk_belu))\n",
        "        f.write(\"(Epoch %d) Train loss %.4f , fk_belu %.18f \\n\" % (epoch, print_loss_avg, fk_belu))    \n",
        "        train_losses.append(print_loss_avg) \n",
        "        fk_belu_train.append(fk_belu)\n",
        "        \n",
        "        # Validation loss\n",
        "        val_loss, fk_belu = evaluate_val_loss(backward_decoder, forward_decoder,teacher_forcing_ratio=0)\n",
        "        print('(Epoch %d) Validation loss %.4f , fk_belu %.18f \\n' % (epoch, val_loss, fk_belu))\n",
        "        f_2.write(\"(Epoch %d) %.4f , fk_belu %.18f \\n\" % (epoch, val_loss, fk_belu))\n",
        "        val_losses.append(val_loss)\n",
        "        fk_belu_val.append(fk_belu)\n",
        "    epoch += 1\n",
        "    save_model(epoch)\n",
        "\n",
        "print(\"Done!\")\n",
        "\n",
        "f.close()\n",
        "f_2.close()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "(Epoch 1)(pair number 100) 6.5798\n",
            "(Epoch 1)(pair number 200) 6.4007\n",
            "(Epoch 1)(pair number 300) 6.3265\n",
            "(Epoch 1)(pair number 400) 6.3122\n",
            "(Epoch 1)(pair number 500) 6.3566\n",
            "(Epoch 1)(pair number 600) 6.4513\n",
            "(Epoch 1)(pair number 700) 6.4674\n",
            "(Epoch 1)(pair number 800) 6.5201\n",
            "(Epoch 1)(pair number 900) 6.5321\n",
            "(Epoch 1)(pair number 1000) 6.5443\n",
            "(Epoch 1) Train loss 6.5371 , fk_belu 0.037927681736526352 \n",
            "\n",
            "(Epoch 1) Validation loss 6.3982 , fk_belu 0.038229808090721316 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 2\n",
            "(Epoch 2)(pair number 100) 6.0590\n",
            "(Epoch 2)(pair number 200) 5.9946\n",
            "(Epoch 2)(pair number 300) 5.9052\n",
            "(Epoch 2)(pair number 400) 5.8751\n",
            "(Epoch 2)(pair number 500) 5.8133\n",
            "(Epoch 2)(pair number 600) 5.8364\n",
            "(Epoch 2)(pair number 700) 5.8043\n",
            "(Epoch 2)(pair number 800) 5.8038\n",
            "(Epoch 2)(pair number 900) 5.7818\n",
            "(Epoch 2)(pair number 1000) 5.7711\n",
            "(Epoch 2) Train loss 5.7548 , fk_belu 0.034323921453290780 \n",
            "\n",
            "(Epoch 2) Validation loss 6.1668 , fk_belu 0.052144855122416303 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 3\n",
            "(Epoch 3)(pair number 100) 5.2025\n",
            "(Epoch 3)(pair number 200) 5.2040\n",
            "(Epoch 3)(pair number 300) 5.1686\n",
            "(Epoch 3)(pair number 400) 5.1151\n",
            "(Epoch 3)(pair number 500) 5.1055\n",
            "(Epoch 3)(pair number 600) 5.3074\n",
            "(Epoch 3)(pair number 700) 5.4208\n",
            "(Epoch 3)(pair number 800) 5.5482\n",
            "(Epoch 3)(pair number 900) 5.6079\n",
            "(Epoch 3)(pair number 1000) 5.6260\n",
            "(Epoch 3) Train loss 5.6218 , fk_belu 0.028754189819699832 \n",
            "\n",
            "(Epoch 3) Validation loss 6.2523 , fk_belu 0.046662943209534589 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 4\n",
            "(Epoch 4)(pair number 100) 5.1651\n",
            "(Epoch 4)(pair number 200) 5.2518\n",
            "(Epoch 4)(pair number 300) 5.2841\n",
            "(Epoch 4)(pair number 400) 5.3074\n",
            "(Epoch 4)(pair number 500) 5.2284\n",
            "(Epoch 4)(pair number 600) 5.2641\n",
            "(Epoch 4)(pair number 700) 5.2647\n",
            "(Epoch 4)(pair number 800) 5.3229\n",
            "(Epoch 4)(pair number 900) 5.3329\n",
            "(Epoch 4)(pair number 1000) 5.3347\n",
            "(Epoch 4) Train loss 5.3285 , fk_belu 0.049597034696985794 \n",
            "\n",
            "(Epoch 4) Validation loss 6.1129 , fk_belu 0.075620757418059070 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 5\n",
            "(Epoch 5)(pair number 100) 4.9307\n",
            "(Epoch 5)(pair number 200) 4.9423\n",
            "(Epoch 5)(pair number 300) 4.9398\n",
            "(Epoch 5)(pair number 400) 5.0058\n",
            "(Epoch 5)(pair number 500) 5.0161\n",
            "(Epoch 5)(pair number 600) 5.0449\n",
            "(Epoch 5)(pair number 700) 5.0441\n",
            "(Epoch 5)(pair number 800) 5.0973\n",
            "(Epoch 5)(pair number 900) 5.1063\n",
            "(Epoch 5)(pair number 1000) 5.1135\n",
            "(Epoch 5) Train loss 5.1026 , fk_belu 0.059668063200903280 \n",
            "\n",
            "(Epoch 5) Validation loss 6.0627 , fk_belu 0.094257834853002750 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 6\n",
            "(Epoch 6)(pair number 100) 4.7462\n",
            "(Epoch 6)(pair number 200) 4.8363\n",
            "(Epoch 6)(pair number 300) 4.8456\n",
            "(Epoch 6)(pair number 400) 4.8401\n",
            "(Epoch 6)(pair number 500) 4.8119\n",
            "(Epoch 6)(pair number 600) 4.8588\n",
            "(Epoch 6)(pair number 700) 4.8268\n",
            "(Epoch 6)(pair number 800) 4.8634\n",
            "(Epoch 6)(pair number 900) 4.8488\n",
            "(Epoch 6)(pair number 1000) 4.8458\n",
            "(Epoch 6) Train loss 4.8255 , fk_belu 0.069506735148044410 \n",
            "\n",
            "(Epoch 6) Validation loss 5.9500 , fk_belu 0.098971860486877117 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 7\n",
            "(Epoch 7)(pair number 100) 4.4696\n",
            "(Epoch 7)(pair number 200) 4.4969\n",
            "(Epoch 7)(pair number 300) 4.4058\n",
            "(Epoch 7)(pair number 400) 4.3548\n",
            "(Epoch 7)(pair number 500) 4.3373\n",
            "(Epoch 7)(pair number 600) 4.3702\n",
            "(Epoch 7)(pair number 700) 4.3444\n",
            "(Epoch 7)(pair number 800) 4.3747\n",
            "(Epoch 7)(pair number 900) 4.3485\n",
            "(Epoch 7)(pair number 1000) 4.3506\n",
            "(Epoch 7) Train loss 4.3384 , fk_belu 0.083599959183771347 \n",
            "\n",
            "(Epoch 7) Validation loss 5.8656 , fk_belu 0.092988719476578052 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 8\n",
            "(Epoch 8)(pair number 100) 3.9063\n",
            "(Epoch 8)(pair number 200) 3.9410\n",
            "(Epoch 8)(pair number 300) 3.8972\n",
            "(Epoch 8)(pair number 400) 3.9031\n",
            "(Epoch 8)(pair number 500) 3.8627\n",
            "(Epoch 8)(pair number 600) 3.9065\n",
            "(Epoch 8)(pair number 700) 3.8826\n",
            "(Epoch 8)(pair number 800) 3.8850\n",
            "(Epoch 8)(pair number 900) 3.8800\n",
            "(Epoch 8)(pair number 1000) 3.8808\n",
            "(Epoch 8) Train loss 3.8625 , fk_belu 0.088249781587229181 \n",
            "\n",
            "(Epoch 8) Validation loss 5.8886 , fk_belu 0.099611660102467525 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 9\n",
            "(Epoch 9)(pair number 100) 3.4130\n",
            "(Epoch 9)(pair number 200) 3.4752\n",
            "(Epoch 9)(pair number 300) 3.4314\n",
            "(Epoch 9)(pair number 400) 3.4059\n",
            "(Epoch 9)(pair number 500) 3.3739\n",
            "(Epoch 9)(pair number 600) 3.4100\n",
            "(Epoch 9)(pair number 700) 3.3674\n",
            "(Epoch 9)(pair number 800) 3.3540\n",
            "(Epoch 9)(pair number 900) 3.3413\n",
            "(Epoch 9)(pair number 1000) 3.3328\n",
            "(Epoch 9) Train loss 3.3087 , fk_belu 0.073177446926197570 \n",
            "\n",
            "(Epoch 9) Validation loss 6.0670 , fk_belu 0.092393807868160788 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 10\n",
            "(Epoch 10)(pair number 100) 3.0730\n",
            "(Epoch 10)(pair number 200) 3.0222\n",
            "(Epoch 10)(pair number 300) 2.9405\n",
            "(Epoch 10)(pair number 400) 2.9274\n",
            "(Epoch 10)(pair number 500) 2.8865\n",
            "(Epoch 10)(pair number 600) 2.9138\n",
            "(Epoch 10)(pair number 700) 2.9055\n",
            "(Epoch 10)(pair number 800) 2.8962\n",
            "(Epoch 10)(pair number 900) 2.8564\n",
            "(Epoch 10)(pair number 1000) 2.8363\n",
            "(Epoch 10) Train loss 2.8208 , fk_belu 0.080298493388363479 \n",
            "\n",
            "(Epoch 10) Validation loss 5.7985 , fk_belu 0.098766950982025981 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 11\n",
            "(Epoch 11)(pair number 100) 2.3200\n",
            "(Epoch 11)(pair number 200) 2.4094\n",
            "(Epoch 11)(pair number 300) 2.3451\n",
            "(Epoch 11)(pair number 400) 2.3058\n",
            "(Epoch 11)(pair number 500) 2.2749\n",
            "(Epoch 11)(pair number 600) 2.3217\n",
            "(Epoch 11)(pair number 700) 2.3284\n",
            "(Epoch 11)(pair number 800) 2.3150\n",
            "(Epoch 11)(pair number 900) 2.3043\n",
            "(Epoch 11)(pair number 1000) 2.2755\n",
            "(Epoch 11) Train loss 2.2603 , fk_belu 0.096904465151307961 \n",
            "\n",
            "(Epoch 11) Validation loss 5.8688 , fk_belu 0.111531329405592808 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 12\n",
            "(Epoch 12)(pair number 100) 2.0682\n",
            "(Epoch 12)(pair number 200) 2.0469\n",
            "(Epoch 12)(pair number 300) 2.0083\n",
            "(Epoch 12)(pair number 400) 1.9845\n",
            "(Epoch 12)(pair number 500) 1.9791\n",
            "(Epoch 12)(pair number 600) 1.9522\n",
            "(Epoch 12)(pair number 700) 1.9300\n",
            "(Epoch 12)(pair number 800) 1.9430\n",
            "(Epoch 12)(pair number 900) 1.9149\n",
            "(Epoch 12)(pair number 1000) 1.8910\n",
            "(Epoch 12) Train loss 1.8753 , fk_belu 0.084172271873746776 \n",
            "\n",
            "(Epoch 12) Validation loss 5.9146 , fk_belu 0.080545924754445983 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 13\n",
            "(Epoch 13)(pair number 100) 1.5991\n",
            "(Epoch 13)(pair number 200) 1.5526\n",
            "(Epoch 13)(pair number 300) 1.5365\n",
            "(Epoch 13)(pair number 400) 1.5386\n",
            "(Epoch 13)(pair number 500) 1.5212\n",
            "(Epoch 13)(pair number 600) 1.5564\n",
            "(Epoch 13)(pair number 700) 1.5388\n",
            "(Epoch 13)(pair number 800) 1.5361\n",
            "(Epoch 13)(pair number 900) 1.5107\n",
            "(Epoch 13)(pair number 1000) 1.4924\n",
            "(Epoch 13) Train loss 1.4921 , fk_belu 0.103330421835756275 \n",
            "\n",
            "(Epoch 13) Validation loss 5.9793 , fk_belu 0.107626067363397970 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 14\n",
            "(Epoch 14)(pair number 100) 1.3789\n",
            "(Epoch 14)(pair number 200) 1.3267\n",
            "(Epoch 14)(pair number 300) 1.2962\n",
            "(Epoch 14)(pair number 400) 1.2695\n",
            "(Epoch 14)(pair number 500) 1.2619\n",
            "(Epoch 14)(pair number 600) 1.2532\n",
            "(Epoch 14)(pair number 700) 1.2443\n",
            "(Epoch 14)(pair number 800) 1.2599\n",
            "(Epoch 14)(pair number 900) 1.2476\n",
            "(Epoch 14)(pair number 1000) 1.2420\n",
            "(Epoch 14) Train loss 1.2381 , fk_belu 0.103798308435395489 \n",
            "\n",
            "(Epoch 14) Validation loss 5.8701 , fk_belu 0.098466940824425392 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 15\n",
            "(Epoch 15)(pair number 100) 1.0126\n",
            "(Epoch 15)(pair number 200) 1.0332\n",
            "(Epoch 15)(pair number 300) 1.0451\n",
            "(Epoch 15)(pair number 400) 1.0214\n",
            "(Epoch 15)(pair number 500) 0.9940\n",
            "(Epoch 15)(pair number 600) 1.0134\n",
            "(Epoch 15)(pair number 700) 1.0055\n",
            "(Epoch 15)(pair number 800) 1.0159\n",
            "(Epoch 15)(pair number 900) 1.0087\n",
            "(Epoch 15)(pair number 1000) 1.0028\n",
            "(Epoch 15) Train loss 1.0000 , fk_belu 0.120281841190415378 \n",
            "\n",
            "(Epoch 15) Validation loss 6.1381 , fk_belu 0.120734619141059193 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 16\n",
            "(Epoch 16)(pair number 100) 0.9283\n",
            "(Epoch 16)(pair number 200) 0.9663\n",
            "(Epoch 16)(pair number 300) 0.9320\n",
            "(Epoch 16)(pair number 400) 0.9024\n",
            "(Epoch 16)(pair number 500) 0.8793\n",
            "(Epoch 16)(pair number 600) 0.8605\n",
            "(Epoch 16)(pair number 700) 0.8475\n",
            "(Epoch 16)(pair number 800) 0.8617\n",
            "(Epoch 16)(pair number 900) 0.8610\n",
            "(Epoch 16)(pair number 1000) 0.8684\n",
            "(Epoch 16) Train loss 0.8652 , fk_belu 0.110435160724310852 \n",
            "\n",
            "(Epoch 16) Validation loss 5.9506 , fk_belu 0.118999561839121193 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 17\n",
            "(Epoch 17)(pair number 100) 0.9021\n",
            "(Epoch 17)(pair number 200) 0.8383\n",
            "(Epoch 17)(pair number 300) 0.7759\n",
            "(Epoch 17)(pair number 400) 0.7322\n",
            "(Epoch 17)(pair number 500) 0.7358\n",
            "(Epoch 17)(pair number 600) 0.7302\n",
            "(Epoch 17)(pair number 700) 0.7201\n",
            "(Epoch 17)(pair number 800) 0.7266\n",
            "(Epoch 17)(pair number 900) 0.7220\n",
            "(Epoch 17)(pair number 1000) 0.7145\n",
            "(Epoch 17) Train loss 0.7076 , fk_belu 0.099511965807342548 \n",
            "\n",
            "(Epoch 17) Validation loss 5.8675 , fk_belu 0.114951977538207031 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 18\n",
            "(Epoch 18)(pair number 100) 0.6497\n",
            "(Epoch 18)(pair number 200) 0.6849\n",
            "(Epoch 18)(pair number 300) 0.6851\n",
            "(Epoch 18)(pair number 400) 0.6735\n",
            "(Epoch 18)(pair number 500) 0.6715\n",
            "(Epoch 18)(pair number 600) 0.6804\n",
            "(Epoch 18)(pair number 700) 0.6827\n",
            "(Epoch 18)(pair number 800) 0.6879\n",
            "(Epoch 18)(pair number 900) 0.6783\n",
            "(Epoch 18)(pair number 1000) 0.6754\n",
            "(Epoch 18) Train loss 0.6687 , fk_belu 0.120192773532521224 \n",
            "\n",
            "(Epoch 18) Validation loss 5.9883 , fk_belu 0.100263795790007243 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 19\n",
            "(Epoch 19)(pair number 100) 0.6679\n",
            "(Epoch 19)(pair number 200) 0.6658\n",
            "(Epoch 19)(pair number 300) 0.6467\n",
            "(Epoch 19)(pair number 400) 0.6249\n",
            "(Epoch 19)(pair number 500) 0.6177\n",
            "(Epoch 19)(pair number 600) 0.6089\n",
            "(Epoch 19)(pair number 700) 0.6078\n",
            "(Epoch 19)(pair number 800) 0.6110\n",
            "(Epoch 19)(pair number 900) 0.5963\n",
            "(Epoch 19)(pair number 1000) 0.5951\n",
            "(Epoch 19) Train loss 0.5952 , fk_belu 0.129123644672072740 \n",
            "\n",
            "(Epoch 19) Validation loss 6.1490 , fk_belu 0.098683691306291371 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 20\n",
            "(Epoch 20)(pair number 100) 0.6347\n",
            "(Epoch 20)(pair number 200) 0.6084\n",
            "(Epoch 20)(pair number 300) 0.5850\n",
            "(Epoch 20)(pair number 400) 0.5683\n",
            "(Epoch 20)(pair number 500) 0.5721\n",
            "(Epoch 20)(pair number 600) 0.5774\n",
            "(Epoch 20)(pair number 700) 0.5694\n",
            "(Epoch 20)(pair number 800) 0.5760\n",
            "(Epoch 20)(pair number 900) 0.5695\n",
            "(Epoch 20)(pair number 1000) 0.5721\n",
            "(Epoch 20) Train loss 0.5651 , fk_belu 0.110621323573743477 \n",
            "\n",
            "(Epoch 20) Validation loss 6.0530 , fk_belu 0.105880456406567444 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 21\n",
            "(Epoch 21)(pair number 100) 0.4579\n",
            "(Epoch 21)(pair number 200) 0.4919\n",
            "(Epoch 21)(pair number 300) 0.5304\n",
            "(Epoch 21)(pair number 400) 0.5276\n",
            "(Epoch 21)(pair number 500) 0.5247\n",
            "(Epoch 21)(pair number 600) 0.5293\n",
            "(Epoch 21)(pair number 700) 0.5312\n",
            "(Epoch 21)(pair number 800) 0.5279\n",
            "(Epoch 21)(pair number 900) 0.5239\n",
            "(Epoch 21)(pair number 1000) 0.5156\n",
            "(Epoch 21) Train loss 0.5133 , fk_belu 0.135829965795189300 \n",
            "\n",
            "(Epoch 21) Validation loss 6.1119 , fk_belu 0.123772011464520840 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 22\n",
            "(Epoch 22)(pair number 100) 0.4202\n",
            "(Epoch 22)(pair number 200) 0.4139\n",
            "(Epoch 22)(pair number 300) 0.4266\n",
            "(Epoch 22)(pair number 400) 0.4350\n",
            "(Epoch 22)(pair number 500) 0.4522\n",
            "(Epoch 22)(pair number 600) 0.4403\n",
            "(Epoch 22)(pair number 700) 0.4509\n",
            "(Epoch 22)(pair number 800) 0.4516\n",
            "(Epoch 22)(pair number 900) 0.4496\n",
            "(Epoch 22)(pair number 1000) 0.4497\n",
            "(Epoch 22) Train loss 0.4503 , fk_belu 0.142537008162602535 \n",
            "\n",
            "(Epoch 22) Validation loss 5.9724 , fk_belu 0.115870780012466396 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 23\n",
            "(Epoch 23)(pair number 100) 0.4576\n",
            "(Epoch 23)(pair number 200) 0.4294\n",
            "(Epoch 23)(pair number 300) 0.4205\n",
            "(Epoch 23)(pair number 400) 0.4233\n",
            "(Epoch 23)(pair number 500) 0.4340\n",
            "(Epoch 23)(pair number 600) 0.4443\n",
            "(Epoch 23)(pair number 700) 0.4402\n",
            "(Epoch 23)(pair number 800) 0.4423\n",
            "(Epoch 23)(pair number 900) 0.4433\n",
            "(Epoch 23)(pair number 1000) 0.4464\n",
            "(Epoch 23) Train loss 0.4425 , fk_belu 0.141689695357144069 \n",
            "\n",
            "(Epoch 23) Validation loss 5.9630 , fk_belu 0.119835908926488335 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 24\n",
            "(Epoch 24)(pair number 100) 0.3475\n",
            "(Epoch 24)(pair number 200) 0.3786\n",
            "(Epoch 24)(pair number 300) 0.3700\n",
            "(Epoch 24)(pair number 400) 0.3670\n",
            "(Epoch 24)(pair number 500) 0.3743\n",
            "(Epoch 24)(pair number 600) 0.3781\n",
            "(Epoch 24)(pair number 700) 0.3836\n",
            "(Epoch 24)(pair number 800) 0.3898\n",
            "(Epoch 24)(pair number 900) 0.3897\n",
            "(Epoch 24)(pair number 1000) 0.3920\n",
            "(Epoch 24) Train loss 0.3924 , fk_belu 0.132368657934251649 \n",
            "\n",
            "(Epoch 24) Validation loss 6.0772 , fk_belu 0.104783014993115667 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 25\n",
            "(Epoch 25)(pair number 100) 0.2819\n",
            "(Epoch 25)(pair number 200) 0.3106\n",
            "(Epoch 25)(pair number 300) 0.3213\n",
            "(Epoch 25)(pair number 400) 0.3556\n",
            "(Epoch 25)(pair number 500) 0.3578\n",
            "(Epoch 25)(pair number 600) 0.3602\n",
            "(Epoch 25)(pair number 700) 0.3618\n",
            "(Epoch 25)(pair number 800) 0.3698\n",
            "(Epoch 25)(pair number 900) 0.3655\n",
            "(Epoch 25)(pair number 1000) 0.3646\n",
            "(Epoch 25) Train loss 0.3613 , fk_belu 0.141746843733537187 \n",
            "\n",
            "(Epoch 25) Validation loss 6.0809 , fk_belu 0.126497943646161765 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 26\n",
            "(Epoch 26)(pair number 100) 0.2892\n",
            "(Epoch 26)(pair number 200) 0.3004\n",
            "(Epoch 26)(pair number 300) 0.3101\n",
            "(Epoch 26)(pair number 400) 0.3273\n",
            "(Epoch 26)(pair number 500) 0.3358\n",
            "(Epoch 26)(pair number 600) 0.3371\n",
            "(Epoch 26)(pair number 700) 0.3358\n",
            "(Epoch 26)(pair number 800) 0.3489\n",
            "(Epoch 26)(pair number 900) 0.3519\n",
            "(Epoch 26)(pair number 1000) 0.3466\n",
            "(Epoch 26) Train loss 0.3451 , fk_belu 0.146157085757176286 \n",
            "\n",
            "(Epoch 26) Validation loss 5.9884 , fk_belu 0.108402763324102730 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 27\n",
            "(Epoch 27)(pair number 100) 0.3650\n",
            "(Epoch 27)(pair number 200) 0.3812\n",
            "(Epoch 27)(pair number 300) 0.3755\n",
            "(Epoch 27)(pair number 400) 0.3436\n",
            "(Epoch 27)(pair number 500) 0.3442\n",
            "(Epoch 27)(pair number 600) 0.3428\n",
            "(Epoch 27)(pair number 700) 0.3344\n",
            "(Epoch 27)(pair number 800) 0.3319\n",
            "(Epoch 27)(pair number 900) 0.3255\n",
            "(Epoch 27)(pair number 1000) 0.3206\n",
            "(Epoch 27) Train loss 0.3144 , fk_belu 0.148889538948422828 \n",
            "\n",
            "(Epoch 27) Validation loss 6.1815 , fk_belu 0.113062969438993827 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 28\n",
            "(Epoch 28)(pair number 100) 0.3139\n",
            "(Epoch 28)(pair number 200) 0.3071\n",
            "(Epoch 28)(pair number 300) 0.3253\n",
            "(Epoch 28)(pair number 400) 0.3322\n",
            "(Epoch 28)(pair number 500) 0.3518\n",
            "(Epoch 28)(pair number 600) 0.3501\n",
            "(Epoch 28)(pair number 700) 0.3480\n",
            "(Epoch 28)(pair number 800) 0.3484\n",
            "(Epoch 28)(pair number 900) 0.3430\n",
            "(Epoch 28)(pair number 1000) 0.3348\n",
            "(Epoch 28) Train loss 0.3355 , fk_belu 0.146719012575855678 \n",
            "\n",
            "(Epoch 28) Validation loss 6.1032 , fk_belu 0.122136380327724198 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 29\n",
            "(Epoch 29)(pair number 100) 0.2844\n",
            "(Epoch 29)(pair number 200) 0.2877\n",
            "(Epoch 29)(pair number 300) 0.2965\n",
            "(Epoch 29)(pair number 400) 0.2802\n",
            "(Epoch 29)(pair number 500) 0.2888\n",
            "(Epoch 29)(pair number 600) 0.2931\n",
            "(Epoch 29)(pair number 700) 0.2941\n",
            "(Epoch 29)(pair number 800) 0.2953\n",
            "(Epoch 29)(pair number 900) 0.2984\n",
            "(Epoch 29)(pair number 1000) 0.3015\n",
            "(Epoch 29) Train loss 0.3019 , fk_belu 0.163135432968390559 \n",
            "\n",
            "(Epoch 29) Validation loss 5.8772 , fk_belu 0.108488232940761042 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 30\n",
            "(Epoch 30)(pair number 100) 0.2721\n",
            "(Epoch 30)(pair number 200) 0.2477\n",
            "(Epoch 30)(pair number 300) 0.2585\n",
            "(Epoch 30)(pair number 400) 0.2629\n",
            "(Epoch 30)(pair number 500) 0.2673\n",
            "(Epoch 30)(pair number 600) 0.2745\n",
            "(Epoch 30)(pair number 700) 0.2753\n",
            "(Epoch 30)(pair number 800) 0.2773\n",
            "(Epoch 30)(pair number 900) 0.2797\n",
            "(Epoch 30)(pair number 1000) 0.2837\n",
            "(Epoch 30) Train loss 0.2817 , fk_belu 0.171755956422650097 \n",
            "\n",
            "(Epoch 30) Validation loss 6.0785 , fk_belu 0.118361186458373180 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 31\n",
            "(Epoch 31)(pair number 100) 0.2104\n",
            "(Epoch 31)(pair number 200) 0.2291\n",
            "(Epoch 31)(pair number 300) 0.2359\n",
            "(Epoch 31)(pair number 400) 0.2331\n",
            "(Epoch 31)(pair number 500) 0.2337\n",
            "(Epoch 31)(pair number 600) 0.2360\n",
            "(Epoch 31)(pair number 700) 0.2380\n",
            "(Epoch 31)(pair number 800) 0.2464\n",
            "(Epoch 31)(pair number 900) 0.2428\n",
            "(Epoch 31)(pair number 1000) 0.2462\n",
            "(Epoch 31) Train loss 0.2483 , fk_belu 0.164515854576655185 \n",
            "\n",
            "(Epoch 31) Validation loss 6.1652 , fk_belu 0.133350712841729141 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 32\n",
            "(Epoch 32)(pair number 100) 0.2963\n",
            "(Epoch 32)(pair number 200) 0.2769\n",
            "(Epoch 32)(pair number 300) 0.2684\n",
            "(Epoch 32)(pair number 400) 0.2657\n",
            "(Epoch 32)(pair number 500) 0.2629\n",
            "(Epoch 32)(pair number 600) 0.2646\n",
            "(Epoch 32)(pair number 700) 0.2576\n",
            "(Epoch 32)(pair number 800) 0.2631\n",
            "(Epoch 32)(pair number 900) 0.2640\n",
            "(Epoch 32)(pair number 1000) 0.2687\n",
            "(Epoch 32) Train loss 0.2710 , fk_belu 0.164097925901953573 \n",
            "\n",
            "(Epoch 32) Validation loss 5.8304 , fk_belu 0.121281316831004279 \n",
            "\n",
            "Saved file:'/content/drive/My Drive/Colab Notebooks/nlp/data/model_params_n_epochs_32.tar for epoch: 33\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Ww1Fr4nTbJ",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7xeTa0Ydor7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "5cc2bb7a-46e0-4724-f9c9-62121d27b6d2"
      },
      "source": [
        "for i in range(3):\n",
        "    evaluate_randomly()\n",
        "    print('\\n')\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation example\n",
            "FKBLEU:  0.06007813382316891\n",
            "לפני השיר שלך אני רוצה להראות לך משהו שהסתתרת מאיתנו היטב !\n",
            "לפני שנראה קצת מהשיר שלך אני רוצה לדבר איתך . מה יש בתמונה הזאת ?\n",
            "<SOS> אבל אנחנו צריכים אתכם פה איתנו שהסתתרת יותר . <EOS>\n",
            "\n",
            "\n",
            "Validation example\n",
            "FKBLEU:  0.24937625272823746\n",
            "כן יהי רצון .\n",
            "כן יהי רצון .\n",
            "<SOS> את כל השירים הכי טובים יעלו לגמר . רצון כל כך שעלו לגמר . <EOS>\n",
            "\n",
            "\n",
            "Validation example\n",
            "FKBLEU:  -2.582373322277759e-07\n",
            "הרצאה הסתכלות מערכתית על השילוב בגן הילדים מטי זכאי משיח אוניברסיטת תל אביב ומכללת בית ברל\n",
            "חשיבה מערכתית כללית על השילוב בגן ילדים מטי זכאי משיח\n",
            "<SOS> בשנת מדינת ישראל תל חי יונתן רוזמן שירלי קני <EOS>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWvLBjfb2irk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "cb647c0f-53ee-4f8b-f7e9-e09af1c69920"
      },
      "source": [
        "for i in range(3):\n",
        "    evaluate_randomly_train()\n",
        "    print('\\n')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train example\n",
            "FKBLEU:  0.14570360576324712\n",
            "אתחיל עם זאת שפתחה את הלילה .\n",
            "אני אתחיל לדבר עם הזמרת ששרה ראשונה היום בערב .\n",
            "<SOS> היום אנחנו פוגשים את הזמרים שפתחה . <EOS>\n",
            "\n",
            "\n",
            "Train example\n",
            "FKBLEU:  0.0007794860660091308\n",
            "הרצאה תכנית רואים רחוק משירות צבאי משמעותי לעבודה בשוק החופשי אפרת סלוניקיו החוג לריפוי בעיסוק הקריה האקדמית אונו\n",
            "רואים רחוק שילוב אנשים עם אוטיזם במקצועות מתאימים בצבא ובעבודה אפרת סלוניקיו\n",
            "<SOS> במדים שילוב אנשים עם מוגבלות לעבודה זכויות של אנשים עם מוגבלות בעבודה לינה זילברברג <EOS>\n",
            "\n",
            "\n",
            "Train example\n",
            "FKBLEU:  0.00707731162126396\n",
            "יו ר יעל רויטמן המרכז לאקדמיה שוויונית הקריה האקדמית אונו .\n",
            "מנחה יעל רויטמן\n",
            "<SOS> מנחה יעל רויטמן <EOS>\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwf4cVqsFYud",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "9f0342d6-6232-48c8-9e42-d96ddb803a81"
      },
      "source": [
        "evaluate_idx(range(min(5,len(X_val))))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation example\n",
            "FKBLEU:  0.24682053905251494\n",
            "ואני מדבר גם על זו שעומדת לידי .\n",
            "ואני מדבר גם על מי שעומדת לידי .\n",
            "SOS> הנהג צריך לעצור את מדינות שעולות לגמר . ואני להצביע . <EOS\n",
            "\n",
            "\n",
            "Validation example\n",
            "FKBLEU:  -0.0003613846706543601\n",
            "הרצאה נרטיבים של הדרה והכלה נצן אלמוג החוג לחינוך וחברה הקריה האקדמית אונו ננה בר אוניברסיטת תל אביב והמרכז ללימודי חירשות\n",
            "סיפורים של הדרה הרחקה והכלה שילוב נצן אלמוג ננה בר\n",
            "SOS> כשירות התאמה רפואית לשירות בצבא תמר נרטיבים מוגבלות ? <EOS\n",
            "\n",
            "\n",
            "Validation example\n",
            "FKBLEU:  0.44432166070322504\n",
            "ד ע י מרפא בעיסוק\n",
            "ד כן מרפא בעיסוק\n",
            "SOS> ב כן מרפא בעיסוק <EOS\n",
            "\n",
            "\n",
            "Validation example\n",
            "FKBLEU:  0.4316044846618002\n",
            "ד איש מקצוע\n",
            "ד איש מקצוע . למשל פיזיותרפיסט\n",
            "SOS> ד איש מקצוע . למשל פיזיותרפיסט ד איש מקצוע . למשל פיזיותרפיסט <EOS\n",
            "\n",
            "\n",
            "Validation example\n",
            "FKBLEU:  2.8772185424952182e-05\n",
            "המדינה תקל את הניידות האישית של אנשים עם מוגבלות כך שיכולו לנוע ממקום למקום בזמן ובאופן שיבחרו במחיר סביר .\n",
            "המדינה תעזור לאנשים עם מוגבלות כך שיוכלו לעבור ממקום למקום בזמן ובצורה שיבחרו במחיר סביר לא יקר מדי .\n",
            "SOS> המדינה תדאג להגן על אנשים עם מוגבלות מכל צורה של ניצול <EOS\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLPQAHIEiq62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "e0ab848c-0b60-41ad-b356-b9c4b218cf2a"
      },
      "source": [
        "# Write all evaluation results on validation data to file\n",
        "evaluate_validation()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "###########################################\n",
            "\n",
            " This is FK-BELU on the validation 0.114667046074687543 \n",
            "\n",
            "\n",
            " This is iBELU on the validation 0.206469299023881198 \n",
            "\n",
            "\n",
            " This is FK on the validation -2.256663094334270259 \n",
            "\n",
            "Wrote those results to file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhi8FUrmFfEO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "cbc75ff9-b1eb-4310-d569-1f1fda944c16"
      },
      "source": [
        "# Plotting Losses\n",
        "file_name = results_dir+'Losses'+files_suffix+'.png'\n",
        "plotter = Plotter(train_losses,\"Train\", val_losses, \"Validation\")\n",
        "plotter.plot(file_name)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-YdCpwq_SSY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "38e9105b-95d2-4a3e-9c35-c7dc5050ebfe"
      },
      "source": [
        "# Plotting FK_Belu\n",
        "file_name = results_dir+'FK_Belu'+files_suffix+'.png'\n",
        "plotter = Plotter(fk_belu_train,\"Train\", fk_belu_val, \"Validation\", \"FK-BELU\")\n",
        "plotter.plot(file_name)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtEaZwNY4g0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}